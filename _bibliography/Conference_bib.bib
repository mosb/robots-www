
@inproceedings{mann_gaussian_2009,
	title = {Gaussian {Processes} for {Prediction} of {Homing} {Pigeon} {Flight} {Trajectories}},
	volume = {1193},
	doi = {10.1063/1.3275635},
	abstract = {We construct and apply a stochastic Gaussian Process (GP) model of flight trajectory generation for pigeons trained to home from specific release sites. The model shows increasing pre- dictive power as the birds become familiar with the sites, mirroring the animal’s learning process. We show how the increasing similarity between successive flight trajectories can be used to infer, with increasing accuracy, an idealised route that captures the repeated spatial aspects of the bird’s flight. We subsequently use techniques associated with reduced-rank GP approximations to objec- tively identify the key waypoints used by each bird to memorise its idiosyncratic habitual route between the release site and the home loft.},
	booktitle = { {AIP} {Conference} {Proceedings}},
	author = {Mann, Richard and Freeman, Robin and Osborne, Michael A. and Garnett, Roman and Meade, Jessica and Armstrong, Chris and Biro, Dora and Guilford, Tim and Roberts, Stephen J. and Goggans, Paul M.},
	year = {2009},
	pages = {360},
	file = {Mann et al_2009_Gaussian Processes for Prediction of Homing Pigeon Flight Trajectories.pdf:/Users/mosb/Google Drive/Zotero/Mann et al_2009_Gaussian Processes for Prediction of Homing Pigeon Flight Trajectories.pdf},
}

@inproceedings{calliess_conservative_2014,
	title = {Conservative collision prediction and avoidance for stochastic trajectories in continuous time and space},
	isbn = {1-4503-2738-9},
	abstract = {Existing work in multi-agent collision prediction and avoidance typically assumes discrete-time tra jectories with Gaussian uncertainty or that are completely deterministic. We propose an approach that allows detection of collisions even between continuous, stochastic trajectories with the only restriction that means and covariances can be computed. To this end, we employ probabilistic bounds to derive criterion functions whose nega tive sign provably is indicative of probable colli sions. For criterion functions that are Lipschitz, an algorithm is provided to rapidly find negative values or prove their absence. We propose an iterative policy-search approach that avoids prior discretisations and yields collision-free trajectories with adjustably high certainty. We test our method with both fixed-priority and auction based protocols for coordinating the iterative plan ning process. Results are provided in collision avoidance simulations of feedback controlled plants.},
	booktitle = {Proceedings of the 2014 international conference on {Autonomous} agents and multi-agent systems ({AAMAS})},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Calliess, Jan-Peter and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2014},
	pages = {1109--1116},
	file = {Calliess et al_2014_Conservative collision prediction and avoidance for stochastic trajectories in.pdf:/Users/mosb/Google Drive/Zotero/Calliess et al_2014_Conservative collision prediction and avoidance for stochastic trajectories in.pdf},
}

@inproceedings{osborne_active_2010,
	title = {Active data selection for sensor networks with faults and changepoints},
	isbn = {1-4244-6695-4},
	abstract = {We describe a Bayesian formalism for the intelligent selection of observations from sensor networks that may intermittently undergo faults or changepoints. Such active data selection is performed with the goal of taking as few observations as necessary in order to maintain a reasonable level of uncertainty about the variables of interest. The presence of faults/changepoints is not always obvious and therefore our algorithm must first detect their occurrence. Having done so, our selection of observations must be appropriately altered. Faults corrupt our observations, reducing their impact; changepoints (abrupt changes in the characteristics of data) may require the transition to an entirely different sampling schedule. Our solution is to employ a Gaussian process formalism that allows for sequential time-series prediction about variables of interest along with a decision theoretic approach to the problem of selecting observations.},
	booktitle = {Advanced {Information} {Networking} and {Applications} ({AINA}), 2010 24th {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Osborne, Michael A. and Garnett, Roman and Roberts, Stephen J.},
	year = {2010},
	pages = {533--540},
	file = {Osborne et al_2010_Active data selection for sensor networks with faults and changepoints.pdf:/Users/mosb/Google Drive/Zotero/Osborne et al_2010_Active data selection for sensor networks with faults and changepoints.pdf},
}

@inproceedings{mathibela_can_2012,
	title = {Can priors be trusted? learning to anticipate roadworks},
	isbn = {1-4673-3064-7},
	abstract = {This paper addresses the question of how much a previously obtained map of a road environment should be trusted for vehicle localisation, during autonomous driving, by assessing the probability that roadworks are being traversed. We compare two formulations of a roadwork prior: one based on Gaussian Process (GP) Classification and the other a more conventional Hidden Markov Model (HMM) in order to model correlations between nearby parts of a vehicle trajectory. Impor- tantly, our formulation allows this prior to be updated efficiently and repeatedly to gain an ever more accurate model of the environment over time. In the absence of, or in addition to, any in-situ observations, information from dedicated web resources can readily be incorporated into the framework. We evaluate our model using real data from an autonomous car and show that although the GP and HMM are roughly commensurate in terms of mapping roadworks, the GP provides a more powerful representation and lower prediction error. Our method allows us to truly map and anticipate roadworks on urban roads.},
	booktitle = {Intelligent {Transportation} {Systems} ({ITSC}), 2012 15th {International} {IEEE} {Conference} on},
	publisher = {IEEE},
	author = {Mathibela, Bonolo and Osborne, Michael A. and Posner, Ingmar and Newman, Paul},
	year = {2012},
	pages = {927--932},
	file = {Mathibela et al_2012_Can priors be trusted.pdf:/Users/mosb/Google Drive/Zotero/Mathibela et al_2012_Can priors be trusted.pdf},
}

@inproceedings{rogers_information_2008,
	title = {Information agents for pervasive sensor networks},
	isbn = {0-7695-3113-X},
	abstract = {In this paper, we describe an information agent, that resides on a mobile computer or personal digital assistant (PDA), that can autonomously acquire sensor readings from pervasive sensor networks (deciding when and which sensor to acquire readings from at any time). Moreover, it can perform a range of information processing tasks including modelling the accuracy of the sensor readings, predicting the value of missing sensor readings, and predicting how the monitored environmental parameters will evolve into the future. Our motivating scenario is the need to provide situational awareness support to first responders at the scene of a large scale incident, and we describe how we use an iterative formulation of a multi-output Gaussian process to build a probabilistic model of the environmental parameters being measured by local sensors, and the correlations and delays that exist between them. We validate our approach using data collected from a network of weather sensors located on the south coast of England.},
	booktitle = {Pervasive {Computing} and {Communications}, 2008. {PerCom} 2008. {Sixth} {Annual} {IEEE} {International} {Conference} on},
	publisher = {IEEE},
	author = {Rogers, Alex and Ramchurn, Sarvapali D. and Jennings, Nicholas R. and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2008},
	pages = {294--299},
	file = {Rogers et al_2008_Information agents for pervasive sensor networks.pdf:/Users/mosb/Google Drive/Zotero/Rogers et al_2008_Information agents for pervasive sensor networks.pdf},
}

@inproceedings{gunter_efficient_2014,
	title = {Efficient {Bayesian} {Nonparametric} {Modelling} of {Structured} {Point} {Processes}},
	url = {http://arxiv.org/abs/1407.6949},
	abstract = {This paper presents a Bayesian generative model for dependent Cox point processes, alongside an efficient inference scheme which scales as if the point processes were modelled independently. We can handle missing data naturally, infer latent structure, and cope with large numbers of observed processes. A further novel contribution enables the model to work effectively in higher dimensional spaces. Using this method, we achieve vastly improved predictive performance on both 2D and 1D real data, validating our structured approach.},
	booktitle = {30th {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI})},
	author = {Gunter, Tom and Lloyd, Chris and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2014},
	file = {Gunter et al_2014_Efficient Bayesian Nonparametric Modelling of Structured Point Processes.pdf:/Users/mosb/Google Drive/Zotero/Gunter et al_2014_Efficient Bayesian Nonparametric Modelling of Structured Point Processes.pdf},
}

@inproceedings{lloyd_latent_2016,
	title = {Latent {Point} {Process} {Allocation}},
	url = {http://jmlr.org/proceedings/papers/v51/lloyd16.html},
	abstract = {We introduce a probabilistic model for the factorisation of continuous Poisson process rate functions. Our model can be thought of as a topic model for Poisson point processes in which each point is assigned to one of a set of latent rate functions that are shared across multiple outputs. We show that the model brings a means of incorporating structure in point process inference beyond the state-of-the-art. We derive an efficient variational inference scheme for the model based on sparse Gaussian processes that scales linearly in the number of data points. Finally, we demonstrate, using examples from spatial and temporal statistics, how the model can be used for discovering hidden structure with greater precision than standard frequentist approaches.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Lloyd, Chris and Gunter, Tom and Osborne, Michael A. and Roberts, Stephen J. and Nickson, Tom},
	year = {2016},
	pages = {389--397},
	file = {Lloyd et al_2016_Latent Point Process Allocation.pdf:/Users/mosb/Google Drive/Zotero/Lloyd et al_2016_Latent Point Process Allocation.pdf},
}

@inproceedings{garnett_active_2014,
	title = {Active {Learning} of {Linear} {Embeddings} for {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1310.6740},
	abstract = {We propose an active learning method for discovering low-dimensional structure in high-dimensional Gaussian process (GP) tasks. Such problems are increasingly frequent and important, but have hitherto presented severe practical difficulties. We further introduce a novel technique for approximately marginalizing GP hyperparameters, yielding marginal predictions robust to hyperparameter mis-specification. Our method offers an efficient means of performing GP regression, quadrature, or Bayesian optimization in high-dimensional spaces.},
	booktitle = {30th {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI})},
	author = {Garnett, Roman and Osborne, Michael A. and Hennig, Philipp},
	year = {2014},
	note = {https://is.gd/orezZh},
	file = {Garnett et al_2014_Active Learning of Linear Embeddings for Gaussian Processes.pdf:/Users/mosb/Google Drive/Zotero/Garnett et al_2014_Active Learning of Linear Embeddings for Gaussian Processes.pdf},
}

@inproceedings{osborne_prediction_2012,
	title = {Prediction and {Fault} {Detection} of {Environmental} {Signals} with {Uncharacterised} {Faults}.},
	abstract = {Many signals of interest are corrupted by faults of an unknown type. We propose an approach that uses Gaussian processes and a general “fault bucket” to capture a priori uncharacterised faults, along with an approximate method for marginalising the potential faultiness of all observations. This gives rise to an efficient, flexible algorithm for the detection and automatic correction of faults. Our method is deployed in the domain of water monitoring and management, where it is able to solve several fault detection, correction, and prediction problems. The method works well despite the fact that the data is plagued with numerous difficulties, including missing observations, multiple discontinuities, nonlinearity and many unanticipated types of fault.},
	booktitle = {The {Association} for the {Advancement} of {Artificial} {Intelligence} {Conference} {On} {Artificial} {Intelligence} ({AAAI})},
	author = {Osborne, Michael A. and Garnett, Roman and Swersky, Kevin and De Freitas, Nando},
	year = {2012},
	note = {https://is.gd/9M0ZYE},
	file = {Osborne et al_2012_Prediction and Fault Detection of Environmental Signals with Uncharacterised.pdf:/Users/mosb/Google Drive/Zotero/Osborne et al_2012_Prediction and Fault Detection of Environmental Signals with Uncharacterised.pdf},
}

@inproceedings{gunter_sampling_2014,
	title = {Sampling for {Inference} in {Probabilistic} {Models} with {Fast} {Bayesian} {Quadrature}},
	url = {http://arxiv.org/abs/1411.0439},
	abstract = {We propose a novel sampling framework for inference in probabilistic models: an active learning approach that converges more quickly (in wall-clock time) than Markov chain Monte Carlo (MCMC) benchmarks. The central challenge in probabilistic inference is numerical integration, to average over ensembles of models or unknown (hyper-)parameters (for example to compute the marginal likelihood or a partition function). MCMC has provided approaches to numerical integration that deliver state-of-the-art inference, but can suffer from sample inefficiency and poor convergence diagnostics. Bayesian quadrature techniques offer a model-based solution to such problems, but their uptake has been hindered by prohibitive computation costs. We introduce a warped model for probabilistic integrands (likelihoods) that are known to be non-negative, permitting a cheap active learning scheme to optimally select sample locations. Our algorithm is demonstrated to offer faster convergence (in seconds) relative to simple Monte Carlo and annealed importance sampling on both synthetic and real-world examples.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Gunter, Tom and Osborne, Michael A. and Garnett, Roman and Hennig, Philipp and Roberts, Stephen J.},
	month = nov,
	year = {2014},
	note = {https://github.com/OxfordML/wsabi},
	keywords = {Statistics - Machine Learning},
	file = {Gunter et al_2014_Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature.pdf:/Users/mosb/Google Drive/Zotero/Gunter et al_2014_Sampling for Inference in Probabilistic Models with Fast Bayesian Quadrature.pdf},
}

@inproceedings{osborne_gaussian_2009,
	title = {Gaussian processes for global optimization},
	abstract = {We introduce a novel Bayesian approach to global optimization using Gaussian processes. We frame the optimization of both noisy and noiseless functions as sequential decision problems, and introduce myopic and non-myopic solutions to them. Here our solutions can be tailored to exactly the degree of confidence we require of them. The use of Gaussian processes allows us to benefit from the incorporation of prior knowledge about our objective function, and also from any derivative observations. Using this latter fact, we introduce an innovative method to combat conditioning problems. Our algorithm demonstrates a significant improvement over its competitors in overall performance across a wide range of canonical test problems.},
	booktitle = {3rd international conference on learning and intelligent optimization ({LION3})},
	author = {Osborne, Michael A. and Garnett, Roman and Roberts, Stephen J.},
	year = {2009},
	pages = {1--15},
	file = {Osborne et al_2009_Gaussian processes for global optimization.pdf:/Users/mosb/Google Drive/Zotero/Osborne et al_2009_Gaussian processes for global optimization.pdf},
}

@inproceedings{garnett_bayesian_2010,
	title = {Bayesian optimization for sensor set selection},
	isbn = {1-60558-988-8},
	doi = {10.1145/1791212.1791238},
	abstract = {We consider the problem of selecting an optimal set of sensors, as determined, for example, by the predictive accuracy of the resulting sensor network. Given an underlying metric between pairs of set elements, we introduce a natural metric between sets of sensors for this task. Using this metric, we can construct covariance functions over sets, and thereby perform Gaussian process inference over a function whose domain is a power set. If the function has additional inputs, our covariances can be readily extended to incorporate them—allowing us to consider, for example, functions over both sets and time. These functions can then be optimized using Gaussian process global optimization (GPGO). We use the root mean squared error (RMSE) of the predictions made using a set of sensors at a particular time as an example of such a function to be optimized; the optimal point specifies the best choice of sensor locations. We demonstrate the resulting method by dynamically selecting the best subset of a given set of weather sensors for the prediction of the air temperature across the United Kingdom.},
	booktitle = {Proceedings of the 9th {ACM}/{IEEE} {International} {Conference} on {Information} {Processing} in {Sensor} {Networks} ({IPSN})},
	publisher = {ACM},
	author = {Garnett, Roman and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2010},
	pages = {209--219},
	file = {Garnett et al_2010_Bayesian optimization for sensor set selection.pdf:/Users/mosb/Google Drive/Zotero/Garnett et al_2010_Bayesian optimization for sensor set selection.pdf},
}

@inproceedings{osborne_bayesian_2012,
	title = {Bayesian quadrature for ratios},
	abstract = {We describe a novel approach to quadrature for ratios of probabilistic integrals, such as are used to compute posterior probabilities. This approach offers performance superior to Monte Carlo methods by exploiting a Bayesian quadrature framework. We improve upon previous Bayesian quadrature techniques by explicitly modelling the nonnegativity of our integrands, and the correlations that exist between them. It offers most where the integrand is multi-modal and expensive to evaluate. We demonstrate the efficacy of our method on data from the Kepler space telescope.},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Osborne, Michael A. and Garnett, Roman and Roberts, Stephen J. and Hart, Christopher and Aigrain, Suzanne and Gibson, Neale},
	year = {2012},
	pages = {832--840},
	file = {Osborne et al_2012_Bayesian quadrature for ratios.pdf:/Users/mosb/Google Drive/Zotero/Osborne et al_2012_Bayesian quadrature for ratios2.pdf},
}

@inproceedings{fischer_recommending_2013,
	title = {Recommending energy tariffs and load shifting based on smart household usage profiling},
	isbn = {1-4503-1965-3},
	abstract = {We present a system and study of personalized energyrelated recommendation. AgentSwitch utilizes electricity usage data collected from users’ households over a period of time to realize a range of smart energy-related recommendations on energy tariffs, load detection and usage shifting. The web service is driven by a third party real-time energy tariff API (uSwitch), an energy data store, a set of algorithms for usage prediction, and appliance-level load disaggregation. We present the system design and user evaluation consisting of interviews and interface walkthroughs. We recruited participants from a previous study during which three months of their household’s energy use was recorded to evaluate personalized recommendations in AgentSwitch. Our contributions are a) a systems architecture for personalized energy services; and b) findings from the evaluation that reveal challenges in designing energy-related recommender systems. In response to the challenges we formulate design recommendations to mitigate barriers to switching tariffs, to incentivize load shifting, and to automate energy management.},
	booktitle = {Proceedings of the 2013 international conference on {Intelligent} user interfaces ({IUI})},
	publisher = {ACM},
	author = {Fischer, Joel E. and Ramchurn, Sarvapali D. and Osborne, Michael A. and Parson, Oliver and Huynh, Trung Dong and Alam, Muddasser and Pantidi, Nadia and Moran, Stuart and Bachour, Khaled and Reece, Steve},
	year = {2013},
	pages = {383--394},
	file = {Fischer et al_2013_Recommending energy tariffs and load shifting based on smart household usage.pdf:/Users/mosb/Google Drive/Zotero/Fischer et al_2013_Recommending energy tariffs and load shifting based on smart household usage.pdf},
}

@inproceedings{osborne_towards_2008,
	address = {Washington, DC, USA},
	series = { {IPSN} '08},
	title = {Towards {Real}-{Time} {Information} {Processing} of {Sensor} {Network} {Data} {Using} {Computationally} {Efficient} {Multi}-output {Gaussian} {Processes}},
	isbn = {978-0-7695-3157-1},
	url = {http://dx.doi.org/10.1109/IPSN.2008.25},
	doi = {10.1109/IPSN.2008.25},
	abstract = {In this paper, we describe a novel, computationally efficient algorithm that facilitates the autonomous acquisition of readings from sensor networks (deciding when and which sensor to acquire readings from at any time), and which can, with minimal domain knowledge, perform a range of information processing tasks including modelling the accuracy of the sensor readings, predicting the value of missing sensor readings, and predicting how the monitored environmental variables will evolve into the future. Our motivating scenario is the need to provide situational awareness support to first responders at the scene of a large scale incident, and to this end, we describe a novel iterative formulation of a multi-output Gaussian process that can build and exploit a probabilistic model of the environmental variables being measured (including the correlations and delays that exist between them). We validate our approach using data collected from a network of weather sensors located on the south coast of England.},
	urldate = {2014-08-30},
	booktitle = {Proceedings of the 7th {International} {Conference} on {Information} {Processing} in {Sensor} {Networks} ({IPSN})},
	publisher = {IEEE Computer Society},
	author = {Osborne, Michael A. and Roberts, Stephen J. and Rogers, Alex and Ramchurn, Sarvapali D. and Jennings, Nicholas R.},
	year = {2008},
	keywords = {Gaussian processes, information processing, sensor network},
	pages = {109--120},
	file = {Osborne et al_2008_Towards Real-Time Information Processing of Sensor Network Data Using.pdf:/Users/mosb/Google Drive/Zotero/Osborne et al_2008_Towards Real-Time Information Processing of Sensor Network Data Using.pdf},
}

@inproceedings{garnett_sequential_2009,
	title = {Sequential {Bayesian} prediction in the presence of changepoints},
	isbn = {1-60558-516-5},
	abstract = {- ence of changepoints. Unlike previous ap- proaches, which focus on the problem of de- tecting and locating changepoints, our al- gorithm focuses on the problem of making predictions even when such changes might be present. We introduce nonstationary co- variance functions to be used in Gaussian process prediction that model such changes, then proceed to demonstrate how to effec- tively manage the hyperparameters associ- ated with those covariance functions. By us- ing Bayesian quadrature, we can integrate out the hyperparameters, allowing us to cal- culate the marginal predictive distribution. Furthermore, if desired, the posterior distri- bution over putative changepoint locations can be calculated as a natural byproduct of our prediction algorithm.},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {ACM},
	author = {Garnett, Roman and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2009},
	pages = {345--352},
	file = {Garnett et al_2009_Sequential Bayesian prediction in the presence of changepoints.pdf:/Users/mosb/Google Drive/Zotero/Garnett et al_2009_Sequential Bayesian prediction in the presence of changepoints.pdf},
}

@inproceedings{ramchurn_agentswitch_2013,
	title = { {AgentSwitch} : towards smart energy tariff selection},
	isbn = {1-4503-1993-9},
	abstract = {In this paper, we present AgentSwitch, a prototype agent-based platform to solve the electricity tariff selection problem. AgentSwitch incorporates novel algorithms to make predictions of hourly energy usage as well as detect (and suggest to the user) deferrable loads that could be shifted to off-peak times to maximise savings. To take advantage of group discounts from energy retailers, we develop a new scalable collective energy purchasing mechanism, based on the Shapley value, that ensures individual members of a collective (interacting through AgentSwitch) fairly share the discounts. To demonstrate the effectiveness of our algorithms we empirically evaluate them individually on real-world data (with up to 3000 homes in the UK) and show that they outperform the state of the art in their domains. Finally, to ensure individual components are accountable in providing recommendations, we provide a novel provenance-tracking service to record the flow of data in the system, and therefore provide users with a means of checking the provenance of suggestions from AgentSwitch and assess their reliability.},
	booktitle = {Proceedings of the 2013 international conference on {Autonomous} agents and multi-agent systems ({AAMAS})},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Ramchurn, Sarvapali D. and Osborne, Michael A. and Parson, Oliver and Rahwan, Talal and Maleki, Sasan and Reece, Steve and Huynh, Trung D. and Alam, Muddasser and Fischer, Joel E. and Rodden, Tom},
	year = {2013},
	pages = {981--988},
	file = {Ramchurn et al_2013_AgentSwitch.pdf:/Users/mosb/Google Drive/Zotero/Ramchurn et al_2013_AgentSwitch.pdf},
}

@inproceedings{briol_frank-wolfe_2015,
	title = {Frank-{Wolfe} {Bayesian} {Quadrature}: {Probabilistic} {Integration} with {Theoretical} {Guarantees}},
	shorttitle = {Frank-{Wolfe} {Bayesian} {Quadrature}},
	url = {http://arxiv.org/abs/1506.02681},
	abstract = {There is renewed interest in formulating integration as an inference problem, motivated by obtaining a full distribution over numerical error that can be propagated through subsequent computation. Current methods, such as Bayesian Quadrature, demonstrate impressive empirical performance but lack theoretical analysis. An important challenge is to reconcile these probabilistic integrators with rigorous convergence guarantees. In this paper, we present the first probabilistic integrator that admits such theoretical treatment, called Frank-Wolfe Bayesian Quadrature (FWBQ). Under FWBQ, convergence to the true value of the integral is shown to be exponential and posterior contraction rates are proven to be superexponential. In simulations, FWBQ is competitive with state-of-the-art methods and out-performs alternatives based on Frank-Wolfe optimisation. Our approach is applied to successfully quantify numerical error in the solution to a challenging model choice problem in cellular biology.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Briol, François-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A.},
	month = jun,
	year = {2015},
	keywords = {Statistics - Machine Learning},
	file = {Briol et al_2015_Frank-Wolfe Bayesian Quadrature.pdf:/Users/mosb/Google Drive/Zotero/Briol et al_2015_Frank-Wolfe Bayesian Quadrature.pdf},
}

@inproceedings{osborne_active_2012,
	title = {Active learning of model evidence using {Bayesian} quadrature},
	abstract = {Numerical integration is a key component of many problems in scientific computing, statistical modelling, and machine learning. Bayesian Quadrature is a modelbased method for numerical integration which, relative to standard Monte Carlo methods, offers increased sample efficiency and a more robust estimate of the uncertainty in the estimated integral. We propose a novel Bayesian Quadrature approach for numerical integration when the integrand is non-negative, such as the case of computing the marginal likelihood, predictive distribution, or normalising constant of a probabilistic model. Our approach approximately marginalises the quadrature model’s hyperparameters in closed form, and introduces an active learning scheme to optimally select function evaluations, as opposed to using Monte Carlo samples. We demonstrate our method on both a number of synthetic benchmarks and a real scientific problem from astronomy.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Osborne, Michael A. and Duvenaud, David K. and Garnett, Roman and Rasmussen, Carl E. and Roberts, Stephen J. and Ghahramani, Zoubin},
	year = {2012},
	pages = {46--54},
	file = {Osborne et al_2012_Active learning of model evidence using Bayesian quadrature.pdf:/Users/mosb/Google Drive/Zotero/Osborne et al_2012_Active learning of model evidence using Bayesian quadrature.pdf},
}

@inproceedings{fitzsimons_improved_2018,
	title = {Improved stochastic trace estimation using mutually unbiased bases},
	url = {http://arxiv.org/abs/1608.00117},
	abstract = {We examine the problem of estimating the trace of a matrix A when given access to an oracle which computes x† A x for an input vector x. We make use of the basis vectors from a set of mutually unbiased bases, widely studied in the field of quantum information processing, in the selection of probing vectors x. This approach offers a new state of the art single shot sampling variance while requiring only O(log(n)) random bits to generate each vector. This significantly improves on traditional methods such as Hutchinson's and Gaussian estimators in terms of the number of random bits required and worst case sample variance.},
	booktitle = {34th {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI})},
	author = {Fitzsimons, Jack K. and Osborne, Michael A. and Roberts, Stephen J. and Fitzsimons, Joe F.},
	year = {2018},
	file = {Fitzsimons et al_2018_Improved stochastic trace estimation using mutually unbiased bases.pdf:/Users/mosb/Google Drive/Zotero/Fitzsimons et al_2018_Improved stochastic trace estimation using mutually unbiased bases.pdf},
}

@inproceedings{sarkar_machine_2016,
	title = {A {Machine} {Learning} {Approach} to the {Prediction} of {Tidal} {Currents}.},
	url = {http://users.ox.ac.uk/~spet1235/ISOPE-2016-TPC-0969-final.pdf},
	abstract = {We propose the use of techniques from Machine Learning for the prediction of tidal currents. The classical methodology of harmonic analysis is widely used in the prediction of tidal currents and computer algorithms based on the method have been used for decades for the purpose. The approach determines parameters by minimizing the difference between the raw data and model output using the least squares optimization approach. However, although the approach is considered to be state-of-the-art, it possesses several drawbacks that can lead to significant prediction errors, especially at locations of fast tidal currents and ’noisy’ tidal signal. In general, careful selection of tidal constituents is required in order to achieve good predictions, and the underlying assumption of stationarity in time can restrict the applicability of the method to particular situations. There is a need for principled approaches which can handle uncertainty and accommodate noise in the data. In this work, we use Gaussian process, a Bayesian non-parametric technique, to predict tidal currents. The overall objective is to take advantage of the recent progress in machine learning to construct a robust yet efficient algorithm. The development can specifically benefit the tidal energy community, aiming to harness energy from location of fast tidal currents.},
	booktitle = {The {Proceedings} of {The} {Twenty}-sixth (2016) {International} {Ocean} {And} {Polar} {Engineering} {Conference}},
	author = {Sarkar, Dripta and Osborne, Michael and Adcock, Thomas},
	year = {2016},
	file = {Sarkar et al_2016_A Machine Learning Approach to the Prediction of Tidal Currents.pdf:/Users/mosb/Google Drive/Zotero/Sarkar et al_2016_A Machine Learning Approach to the Prediction of Tidal Currents.pdf},
}

@inproceedings{gonzalez_glasses:_2016,
	title = { {GLASSES}: {Relieving} {The} {Myopia} {Of} {Bayesian} {Optimisation}},
	shorttitle = { {GLASSES}},
	url = {http://jmlr.org/proceedings/papers/v51/gonzalez16b.html},
	abstract = {We present GLASSES: Global optimisation with Look-Ahead through Stochastic
Simulation and Expected-loss Search. The majority of global optimisation
approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. Our novel algorithm, GLASSES, permits the consideration of dozens of evaluations into the future.  This is done by approximating the ideal look-ahead loss function, which is expensive to evaluate, by a cheaper alternative in which the future steps of the algorithm are simulated beforehand. An Expectation Propagation algorithm is used to compute the expected value of the loss. We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests.},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Gonzalez, Javier and Osborne, Michael A. and Lawrence, Neil},
	year = {2016},
	note = {https://github.com/SheffieldML/GPyOpt/tree/GLASSES},
	pages = {790--799},
	file = {Gonzalez et al_2016_GLASSES.pdf:/Users/mosb/Google Drive/Zotero/Gonzalez et al_2016_GLASSES.pdf},
}

@inproceedings{cutajar_preconditioning_2016,
	title = {Preconditioning {Kernel} {Matrices}},
	url = {http://arxiv.org/abs/1602.06693},
	abstract = {The computational and storage complexity of kernel machines presents the primary barrier to their scaling to large, modern, datasets. A common way to tackle the scalability issue is to use the conjugate gradient algorithm, which relieves the constraints on both storage (the kernel matrix need not be stored) and computation (both stochastic gradients and parallelization can be used). Even so, conjugate gradient is not without its own issues: the conditioning of kernel matrices is often such that conjugate gradients will have poor convergence in practice. Preconditioning is a common approach to alleviating this issue. Here we propose preconditioned conjugate gradients for kernel machines, and develop a broad range of preconditioners particularly useful for kernel matrices. We describe a scalable approach to both solving kernel machines and learning their hyperparameters. We show this approach is exact in the limit of iterations and outperforms state-of-the-art approximations for a given computational budget.},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning} ({ICML})},
	author = {Cutajar, Kurt and Osborne, Michael A. and Cunningham, John P. and Filippone, Maurizio},
	month = feb,
	year = {2016},
	note = {https://is.gd/365blF},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
	file = {Cutajar et al_2016_Preconditioning Kernel Matrices.pdf:/Users/mosb/Google Drive/Zotero/Cutajar et al_2016_Preconditioning Kernel Matrices.pdf},
}

@inproceedings{lloyd_variational_2015,
	title = {Variational {Inference} for {Gaussian} {Process} {Modulated} {Poisson} {Processes}},
	url = {http://jmlr.org/proceedings/papers/v37/lloyd15.html},
	abstract = {We present the first fully variational Bayesian inference scheme for continuous Gaussian-process-modulated Poisson processes. Such point processes are used in a variety of domains, including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our scheme: requires no discretisation of the domain; scales linearly in the number of observed events; and is many orders of magnitude faster than previous sampling based approaches. The resulting algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster data and in the prediction of Malaria incidences in Kenya.},
	booktitle = {Proceedings of {The} 32nd {International} {Conference} on {Machine} {Learning} ({ICML})},
	author = {Lloyd, Chris and Gunter, Tom and Osborne, Michael A. and Roberts, Stephen},
	year = {2015},
	pages = {1814--1822},
	file = {Lloyd et al_2015_Variational Inference for Gaussian Process Modulated Poisson Processes.pdf:/Users/mosb/Google Drive/Zotero/Lloyd et al_2015_Variational Inference for Gaussian Process Modulated Poisson Processes.pdf},
}

@inproceedings{riar_energy_2016,
	title = {Energy management of a microgrid: {Compensating} for the difference between the real and predicted output power of photovoltaics},
	shorttitle = {Energy management of a microgrid},
	doi = {10.1109/PEDG.2016.7527042},
	abstract = {An increasing awareness of energy efficiency has led to the development of several improved converter topologies, semiconductor devices and control schemes for distributed energy resources, and, particularly, for microgrids. Recent advances in energy management systems (EMS) for microgrids have improved upon existing methods in several aspects, including prediction of power generated by photovoltaics (PV), and optimal management of electrical energy storage. However, the actual generated PV power may deviate from predictions for several reasons, such as rapid cloud changes or system faults. This paper contributes to the ongoing research on EMS control schemes by proposing a model predictive control (MPC) scheme that adapts to the difference between the actual and predicted output power of PV. The key benefit of this approach is its ability to rapidly adapt to varying operating conditions of the PV without increasing the computational burden of a typical MPC scheme. The feasibility of the scheme is demonstrated using simulations of 5 kW microgrid system compromising a 5 kW/400 Ah battery, 10 kW PV and 5 kW grid/load connection. The proposed scheme reduces variations in the state of charge (SOC) of a battery. The proposed scheme also reduces the energy taken from grid and this improvement in performance is a function of the difference between the actual and the predicted power.},
	booktitle = {2016 {IEEE} 7th {International} {Symposium} on {Power} {Electronics} for {Distributed} {Generation} {Systems} ({PEDG})},
	author = {Riar, Baljit and Lee, Jaehwa and Tosi, Alessandra and Duncan, Stephen and Osborne, Michael A. and Howey, David},
	month = jun,
	year = {2016},
	keywords = {Batteries, Battery storage system, EMS control schemes, Energy management, MPC scheme, Microgrids, Model predictive control, Optimisation, Optimization, PV, Power generation, Predictive control, State of charge, battery, battery state of charge variations reduction, computational burden, converter topology, distributed energy resource, distributed power generation, electrical energy storage optimal management, energy conservation, energy efficiency awareness, energy management system, energy management systems, energy storage, grid-load connection, microgrid, microgrid energy management system, model predictive control scheme, photovoltaic (PV) system, photovoltaic power systems, photovoltaics, power 10 kW, power 5 kW, power convertors, power generation control, power generation prediction, power grids, secondary cells, semiconductor device},
	pages = {1--7},
	file = {Riar et al_2016_Energy management of a microgrid.pdf:/Users/mosb/Google Drive/Zotero/Riar et al_2016_Energy management of a microgrid.pdf},
}

@inproceedings{rainforth_bayesian_2016,
	title = {Bayesian {Optimization} for {Probabilistic} {Programs}},
	url = {http://papers.nips.cc/paper/6421-bayesian-optimization-for-probabilistic-programs.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	publisher = {Curran Associates, Inc.},
	author = {Rainforth, Tom and Le, Tuan Anh and van de Meent, Jan-Willem and Osborne, Michael A and Wood, Frank},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	note = {https://github.com/probprog/bopp},
	pages = {280--288},
	file = {Rainforth et al_2016_Bayesian Optimization for Probabilistic Programs.pdf:/Users/mosb/Google Drive/Zotero/Rainforth et al_2016_Bayesian Optimization for Probabilistic Programs.pdf},
}

@inproceedings{fruehwirt_bayesian_2017,
	title = {Bayesian {Gaussian} {Process} {Classification} from {Event}-{Related} {Brain} {Potentials} in {Alzheimer}’s {Disease}},
	volume = {10259},
	url = {https://is.gd/z5lLxE},
	abstract = {Event-related potentials (ERPs) have been shown to reflect neurodegenerative processes in Alzheimer’s disease (AD) and might qualify as non-invasive and cost-effective markers to facilitate the objectivization of AD assessment in daily clinical practice. Lately, the combination of multivariate pattern analysis (MVPA) and Gaussian process classification (GPC) has gained interest in the neuroscientific community. Here, we demonstrate how a MVPA-GPC approach can be applied to electrophysiological data. Furthermore, in order to account for the temporal information of ERPs, we develop a novel method that integrates interregional synchrony of ERP time signatures. By using real-life ERP recordings of a prospective AD cohort study (PRODEM), we empirically investigate the usefulness of the proposed framework to build neurophysiological markers for single subject classification tasks. GPC outperforms the probabilistic reference method in both tasks, with the highest AUC overall (0.802) being achieved using the new spatiotemporal method in the prediction of rapid cognitive decline.},
	booktitle = {Artificial {Intelligence} in {Medicine}: 16th {Conference} on {Artificial} {Intelligence} in {Medicine}, {AIME} 2017, {Vienna}, {Austria}, {June} 21-24, 2017, {Proceedings}},
	publisher = {Springer},
	author = {Fruehwirt, Wolfgang and Zhang, Pengfei and Gerstgrasser, Matthias and Grossegger, Dieter and Schmidt, Reinhold and Benke, Thomas and Dal-Bianco, Peter and Ransmayr, Gerhard and Weydemann, Leonard and Garn, Heinrich and Waser, Markus and Osborne, Michael A and Dorffner, Georg},
	year = {2017},
	pages = {65},
	file = {Fruehwirt et al_2017_Bayesian Gaussian Process Classification from Event-Related Brain Potentials in.pdf:/Users/mosb/Google Drive/Zotero/Fruehwirt et al_2017_Bayesian Gaussian Process Classification from Event-Related Brain Potentials in.pdf},
}

@inproceedings{fitzsimons_entropic_2017,
	title = {Entropic {Trace} {Estimates} for {Log} {Determinants}},
	url = {https://arxiv.org/abs/1704.07223},
	abstract = {The scalable calculation of matrix determinants has been a bottleneck to the widespread application of many machine learning methods such as determinantal point processes, Gaussian processes, generalised Markov random  elds, graph models and many others. In this work, we estimate log determinants under the framework of maximum entropy, given information in the form of moment constraints from stochastic trace estimation. The estimates demonstrate a signi cant improvement on state-of-the-art alternative methods, as shown on a wide variety of matrices from the SparseSuite Matrix Collection. By taking the example of a general Markov random  eld, we also demonstrate how this approach can signifcantly accelerate inference in large-scale learning methods involving the log determinant.},
	urldate = {2017-06-21},
	booktitle = { {ECML}/{PKDD} 2017, {European} {Conference} on {Machine} {Learning} and {Principles} and {Practice} of {Knowledge} {Discovery} in {Databases}, {September} 18-22, 2017, {Skopje}, {Macedonia}},
	author = {Fitzsimons, Jack and Granziol, Diego and Cutajar, Kurt and Osborne, Michael and Filippone, Maurizio and Roberts, Stephen},
	year = {2017},
	file = {Fitzsimons et al_2017_Entropic Trace Estimates for Log Determinants.pdf:/Users/mosb/Google Drive/Zotero/Fitzsimons et al_2017_Entropic Trace Estimates for Log Determinants.pdf},
}

@inproceedings{fitzsimons_bayesian_2017,
	title = {Bayesian {Inference} of {Log} {Determinants}},
	url = {https://arxiv.org/abs/1704.01445},
	abstract = {The log-determinant of a kernel matrix appears in a variety of machine learning problems, ranging from determinantal point processes and generalized Markov random fields, through to the training of Gaussian processes. Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousand. In the spirit of probabilistic numerics, we reinterpret the problem of computing the log-determinant as a Bayesian inference problem. In particular, we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget. Beyond its novelty and theoretic appeal, the performance of our proposal is competitive with state-of-the-art approaches to approximating the log-determinant, while also quantifying the uncertainty due to budget-constrained evidence.},
	urldate = {2017-06-21},
	booktitle = {Uncertainty in {Artificial} {Intelligence} ({UAI})},
	author = {Fitzsimons, Jack and Cutajar, Kurt and Osborne, Michael and Roberts, Stephen and Filippone, Maurizio},
	year = {2017},
	file = {Fitzsimons et al_2017_Bayesian Inference of Log Determinants.pdf:/Users/mosb/Google Drive/Zotero/Fitzsimons et al_2017_Bayesian Inference of Log Determinants.pdf},
}

@inproceedings{bewsher_distribution_2017,
	title = {Distribution of {Gaussian} {Process} {Arc} {Lengths}},
	url = {http://proceedings.mlr.press/v54/bewsher17a.html},
	abstract = {We present the first treatment of the arc length of the Gaussian Process (gp) with more than a single output dimension. Gps are commonly used for tasks such as trajectory modelling, where path length is a crucial quantity of interest. Previously, only paths in one dimension have been considered, with no theoretical consideration of higher dimensional problems. We fill the gap in the existing literature by deriving the moments of the arc length for a stationary gp with multiple output dimensions. A new method is used to derive the mean of a one-dimensional gp over a finite interval, by considering the distribution of the arc length integrand. This technique is used to derive an approximate distribution over the arc length of a vector valued gp in Rn by moment matching the distribution. Numerical simulations confirm our theoretical derivations.},
	language = {en},
	urldate = {2017-06-21},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Bewsher, Justin and Tosi, Alessandra and Osborne, Michael and Roberts, Stephen},
	month = apr,
	year = {2017},
	file = {Bewsher et al_2017_Distribution of Gaussian Process Arc Lengths.pdf:/Users/mosb/Google Drive/Zotero/Bewsher et al_2017_Distribution of Gaussian Process Arc Lengths.pdf},
}

@inproceedings{paul_alternating_2018,
	title = {Alternating {Optimisation} and {Quadrature} for {Robust} {Control}},
	url = {http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/paulaaai18.pdf},
	abstract = {Bayesian optimisation has been successfully applied to a variety of reinforcement learning problems. However, the traditional approach for learning optimal policies in simulators does not utilise the opportunity to improve learning by adjusting certain environment variables: state features that are unobservable and randomly determined by the environment in a physical setting but are controllable in a simulator. This paper considers the problem of finding a robust policy while taking into account the impact of environment variables. We present Alternating Optimisation and Quadrature (ALOQ), which uses Bayesian optimisation and Bayesian quadrature to address such settings. ALOQ is robust to the presence of significant rare events, which may not be observable under random sampling, but play a substantial role in determining the optimal policy. Experimental results across different domains show that ALOQ can learn more efficiently and robustly than existing methods.},
	urldate = {2017-12-21},
	booktitle = {Proceedings of the {Thirty}-{Second} {AAAI} {Conference} on {Artificial} {Intelligence} ({AAAI})},
	author = {Paul, Supratik and Chatzilygeroudis, Konstantinos and Ciosek, Kamil and Mouret, Jean-Baptiste and Osborne, Michael A. and Whiteson, Shimon},
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
	file = {Paul et al_2018_Alternating Optimisation and Quadrature for Robust Control.pdf:/Users/mosb/Google Drive/Zotero/Paul et al_2018_Alternating Optimisation and Quadrature for Robust Control.pdf},
}

@inproceedings{abbati_adageo_2018,
	title = { {AdaGeo} : {Adaptive} {Geometric} {Learning} for {Optimization} and {Sampling}},
	shorttitle = { {AdaGeo}},
	url = {http://proceedings.mlr.press/v84/abbati18a.html},
	abstract = {Gradient-based optimization and Markov Chain Monte Carlo sampling can be found at the heart of several machine learning methods. In high-dimensional settings, well-known issues such as slow-mixing,...},
	language = {en},
	urldate = {2018-04-13},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Abbati, Gabriele and Tosi, Alessandra and Osborne, Michael and Flaxman, Seth},
	month = mar,
	year = {2018},
	pages = {226--234},
	file = {Abbati et al_2018_AdaGeo.pdf:/Users/mosb/Google Drive/Zotero/Abbati et al_2018_AdaGeo.pdf},
}

@inproceedings{richardson_battery_2017,
	title = {Battery {Capacity} {Estimation} {From} {Partial}-{Charging} {Data} {Using} {Gaussian} {Process} {Regression}},
	url = {http://dx.doi.org/10.1115/DSCC2017-5365},
	doi = {10.1115/DSCC2017-5365},
	abstract = {Accurate on-board capacity estimation is of critical importance in lithium-ion battery applications. Battery charging/discharging often occurs under a constant current load, and hence voltage vs. time measurements under this condition may be accessible in practice. This paper presents a novel diagnostic technique, Gaussian Process regression for In-situ Capacity Estimation (GP-ICE), which is capable of estimating the battery capacity using voltage vs. time measurements over short periods of galvanostatic operation.The approach uses Gaussian process regression to map from voltage values at a selection of uniformly distributed times, to cell capacity. Unlike previous works, GP-ICE does not rely on interpreting the voltage-time data through the lens of Incremental Capacity (IC) or Differential Voltage (DV) analysis. This overcomes both the need to differentiate the voltage-time data (a process which amplifies measurement noise), and the requirement that the range of voltage measurements encompasses the peaks in the IC/DV curves. Rather, GP-ICE gives insight into which portions of the voltage range are most informative about the capacity for a particular cell. We apply GP-ICE to a dataset of 8 cells, which were aged by repeated application of an ARTEMIS urban drive cycle. Within certain voltage ranges, as little as 10 seconds of charge data is sufficient to enable capacity estimates with ∼ 2\% RMSE.},
	urldate = {2018-04-16},
	booktitle = { {ASME} 2017 {Dynamic} {Systems} and {Control} {Conference}},
	author = {Richardson, Robert R. and Birkl, Christoph R. and Osborne, Michael A. and Howey, David A.},
	month = oct,
	year = {2017},
	file = {Richardson et al_2017_Battery Capacity Estimation From Partial-Charging Data Using Gaussian Process.pdf:/Users/mosb/Google Drive/Zotero/Richardson et al_2017_Battery Capacity Estimation From Partial-Charging Data Using Gaussian Process.pdf},
}

@inproceedings{mcleod_optimization_2018,
	title = {Optimization, fast and slow: optimally switching between local and {Bayesian} optimization},
	shorttitle = {Optimization, fast and slow},
	url = {http://arxiv.org/abs/1805.08610},
	abstract = {We develop the first Bayesian Optimization algorithm, BLOSSOM, which selects between multiple alternative acquisition functions and traditional local optimization at each step. This is combined with a novel stopping condition based on expected regret. This pairing allows us to obtain the best characteristics of both local and Bayesian optimization, making efficient use of function evaluations while yielding superior convergence to the global minimum on a selection of optimization problems, and also halting optimization once a principled and intuitive stopping condition has been fulfilled.},
	urldate = {2018-05-23},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning} ({ICML})},
	author = {McLeod, Mark and Osborne, Michael A. and Roberts, Stephen J.},
	month = may,
	year = {2018},
	note = {https://github.com/markm541374/gpbo},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {McLeod et al_2018_Optimization, fast and slow.pdf:/Users/mosb/Google Drive/Zotero/McLeod et al_2018_Optimization, fast and slow.pdf},
}

@inproceedings{ru_fast_2018,
	title = {Fast {Information}-theoretic {Bayesian} {Optimisation}},
	url = {http://arxiv.org/abs/1711.00673},
	abstract = {Information-theoretic Bayesian optimisation techniques have demonstrated state-of-the-art performance in tackling important global optimisation problems. However, current information-theoretic approaches require many approximations in implementation, introduce often-prohibitive computational overhead and limit the choice of kernels available to model the objective. We develop a fast information-theoretic Bayesian Optimisation method, FITBO, that avoids the need for sampling the global minimiser, thus significantly reducing computational overhead. Moreover, in comparison with existing approaches, our method faces fewer constraints on kernel choice and enjoys the merits of dealing with the output space. We demonstrate empirically that FITBO inherits the performance associated with information-theoretic Bayesian optimisation, while being even faster than simpler Bayesian optimisation approaches, such as Expected Improvement.},
	urldate = {2018-06-07},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning} ({ICML})},
	author = {Ru, Binxin and McLeod, Mark and Granziol, Diego and Osborne, Michael A.},
	year = {2018},
	note = {https://github.com/rubinxin/FITBO},
	keywords = {Statistics - Machine Learning},
	file = {Ru et al_2018_Fast Information-theoretic Bayesian Optimisation.pdf:/Users/mosb/Google Drive/Zotero/Ru et al_2018_Fast Information-theoretic Bayesian Optimisation.pdf},
}

@inproceedings{abbati_ares_2019,
	title = { {AReS} and {MaRS} {Adversarial} and {MMD}-{Minimizing} {Regression} for {SDEs}},
	url = {http://proceedings.mlr.press/v97/abbati19a.html},
	abstract = {Stochastic differential equations are an important modeling class in many disciplines. Consequently, there exist many methods relying on various discretization and numerical integration schemes. In...},
	language = {en},
	urldate = {2019-09-29},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Abbati, Gabriele and Wenk, Philippe and Osborne, Michael A. and Krause, Andreas and Schölkopf, Bernhard and Bauer, Stefan},
	month = may,
	year = {2019},
	note = {https://github.com/gabb7/AReS-MaRS},
	pages = {1--10},
	file = {Abbati et al_2019_AReS and MaRS Adversarial and MMD-Minimizing Regression for SDEs.pdf:/Users/mosb/Google Drive/Zotero/Abbati et al_2019_AReS and MaRS Adversarial and MMD-Minimizing Regression for SDEs.pdf},
}

@inproceedings{alvi_asynchronous_2019,
	title = {Asynchronous {Batch} {Bayesian} {Optimisation} with {Improved} {Local} {Penalisation}},
	url = {http://proceedings.mlr.press/v97/alvi19a.html},
	abstract = {Batch Bayesian optimisation (BO) has been successfully applied to hyperparameter tuning using parallel computing, but it is wasteful of resources: workers that complete jobs ahead of others are lef...},
	language = {en},
	urldate = {2019-09-29},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Alvi, Ahsan and Ru, Binxin and Calliess, Jan-Peter and Roberts, Stephen and Osborne, Michael A.},
	month = may,
	year = {2019},
	note = {https://github.com/a5a/asynchronous-BO},
	pages = {253--262},
	file = {Alvi et al_2019_Asynchronous Batch Bayesian Optimisation with Improved Local Penalisation.pdf:/Users/mosb/Google Drive/Zotero/Alvi et al_2019_Asynchronous Batch Bayesian Optimisation with Improved Local Penalisation.pdf},
}

@inproceedings{chai_automated_2019,
	title = {Automated {Model} {Selection} with {Bayesian} {Quadrature}},
	url = {http://proceedings.mlr.press/v97/chai19a.html},
	abstract = {We present a novel technique for tailoring Bayesian quadrature (BQ) to model selection. The state-of-the-art for comparing the evidence of multiple models relies on Monte Carlo methods, which conve...},
	language = {en},
	urldate = {2019-09-29},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Chai, Henry and Ton, Jean-Francois and Osborne, Michael A. and Garnett, Roman},
	month = may,
	year = {2019},
	pages = {931--940},
	file = {Chai et al_2019_Automated Model Selection with Bayesian Quadrature.pdf:/Users/mosb/Google Drive/Zotero/Chai et al_2019_Automated Model Selection with Bayesian Quadrature.pdf},
}

@inproceedings{paul_fingerprint_2019,
	title = {Fingerprint {Policy} {Optimisation} for {Robust} {Reinforcement} {Learning}},
	url = {http://proceedings.mlr.press/v97/paul19a.html},
	abstract = {Policy gradient methods ignore the potential value of adjusting environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but are con...},
	language = {en},
	urldate = {2019-09-29},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Paul, Supratik and Osborne, Michael A. and Whiteson, Shimon},
	month = may,
	year = {2019},
	pages = {5082--5091},
	file = {Paul et al_2019_Fingerprint Policy Optimisation for Robust Reinforcement Learning.pdf:/Users/mosb/Google Drive/Zotero/Paul et al_2019_Fingerprint Policy Optimisation for Robust Reinforcement Learning.pdf},
}

@inproceedings{wagstaff_limitations_2019,
	title = {On the {Limitations} of {Representing} {Functions} on {Sets}},
	url = {http://proceedings.mlr.press/v97/wagstaff19a.html},
	abstract = {Recent work on the representation of functions on sets has considered the use of summation in a latent space to enforce permutation invariance. In particular, it has been conjectured that the dimen...},
	language = {en},
	urldate = {2019-09-29},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Wagstaff, Edward and Fuchs, Fabian and Engelcke, Martin and Posner, Ingmar and Osborne, Michael A.},
	month = may,
	year = {2019},
	pages = {6487--6494},
	file = {Wagstaff et al_2019_On the Limitations of Representing Functions on Sets.pdf:/Users/mosb/Google Drive/Zotero/Wagstaff et al_2019_On the Limitations of Representing Functions on Sets.pdf},
}

@inproceedings{duckworth_inferring_2019,
	address = {Honolulu, HI, USA},
	series = { {AIES} '19},
	title = {Inferring {Work} {Task} {Automatability} from {AI} {Expert} {Evidence}},
	isbn = {978-1-4503-6324-2},
	url = {https://doi.org/10.1145/3306618.3314247},
	doi = {10.1145/3306618.3314247},
	abstract = {Despite growing alarm about machine learning technologies automating jobs, there is little good evidence on what activities can be automated using such technologies. We contribute the first dataset of its kind by surveying over 150 top academics and industry experts in machine learning, robotics and AI, receiving over 4,500 ratings of how automatable specific tasks are today. We present a probabilistic machine learning model to learn the patterns connecting expert estimates of task automatability and the skills, knowledge and abilities required to perform those tasks. Our model infers the automatability of over 2,000 work activities, and we show how automation differs across types of activities and types of occupations. Sensitivity analysis identifies the specific skills, knowledge and abilities of activities that drive higher or lower automatability. We provide quantitative evidence of what is perceived to be automatable using the state-of-the-art in machine learning technology. We consider the societal impacts of these results and of task-level approaches.},
	urldate = {2020-01-24},
	booktitle = {Proceedings of the 2019 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {Association for Computing Machinery},
	author = {Duckworth, Paul and Graham, Logan and Osborne, Michael},
	month = jan,
	year = {2019},
	note = {https://drive.google.com/drive/folders/1gvS9fQ2gG9y5VrPyvRWKCh0yxdsqI9V2},
	keywords = {automation, bayesian machine learning, interpretable machine learning, labor economics, open datasets},
	pages = {485--491},
	file = {Duckworth et al_2019_Inferring Work Task Automatability from AI Expert Evidence.pdf:/Users/mosb/Google Drive/Zotero/Duckworth et al_2019_Inferring Work Task Automatability from AI Expert Evidence.pdf},
}

@inproceedings{farquhar_radial_2020,
	title = {Radial {Bayesian} {Neural} {Networks}: {Beyond} {Discrete} {Support} {In} {Large}-{Scale} {Bayesian} {Deep} {Learning}},
	shorttitle = {Radial {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1907.00865},
	abstract = {We propose Radial Bayesian Neural Networks (BNNs): a variational approximate posterior for BNNs which scales well to large models while maintaining a distribution over weight-space with full support. Other scalable Bayesian deep learning methods, like MC dropout or deep ensembles, have discrete support-they assign zero probability to almost all of the weight-space. Unlike these discrete support methods, Radial BNNs' full support makes them suitable for use as a prior for sequential inference. In addition, they solve the conceptual challenges with the a priori implausibility of weight distributions with discrete support. The Radial BNN is motivated by avoiding a sampling problem in 'mean-field' variational inference (MFVI) caused by the so-called 'soap-bubble' pathology of multivariate Gaussians. We show that, unlike MFVI, Radial BNNs are robust to hyperparameters and can be efficiently applied to a challenging real-world medical application without needing ad-hoc tweaks and intensive tuning. In fact, in this setting Radial BNNs out-perform discrete-support methods like MC dropout. Lastly, by using Radial BNNs as a theoretically principled, robust alternative to MFVI we make significant strides in a Bayesian continual learning evaluation.},
	urldate = {2020-05-16},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics} ({AISTATS})},
	author = {Farquhar, Sebastian and Osborne, Michael and Gal, Yarin},
	year = {2020},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Farquhar et al_2020_Radial Bayesian Neural Networks.pdf:/Users/mosb/Google Drive/Zotero/Farquhar et al_2020_Radial Bayesian Neural Networks.pdf},
}

@inproceedings{nguyen_optimal_2021,
	title = {Optimal {Transport} {Kernels} for {Sequential} and {Parallel} {Neural} {Architecture} {Search}},
	url = {http://proceedings.mlr.press/v139/nguyen21d.html},
	abstract = {Neural architecture search (NAS) automates the design of deep neural networks. One of the main challenges in searching complex and non-continuous architectures is to compare the similarity of networks that the conventional Euclidean metric may fail to capture. Optimal transport (OT) is resilient to such complex structure by considering the minimal cost for transporting a network into another. However, the OT is generally not negative definite which may limit its ability to build the positive-definite kernels required in many kernel-dependent frameworks. Building upon tree-Wasserstein (TW), which is a negative definite variant of OT, we develop a novel discrepancy for neural architectures, and demonstrate it within a Gaussian process surrogate model for the sequential NAS settings. Furthermore, we derive a novel parallel NAS, using quality k-determinantal point process on the GP posterior, to select diverse and high-performing architectures from a discrete set of candidates. Empirically, we demonstrate that our TW-based approaches outperform other baselines in both sequential and parallel NAS.},
	language = {en},
	urldate = {2021-08-11},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {PMLR},
	author = {Nguyen, Vu and Le, Tam and Yamada, Makoto and Osborne, Michael A.},
	month = jul,
	year = {2021},
	note = {https://github.com/ntienvu/TW\_NAS},
	pages = {8084--8095},
	file = {Nguyen et al_2021_Optimal Transport Kernels for Sequential and Parallel Neural Architecture Search.pdf:/Users/mosb/Google Drive/Zotero/Nguyen et al_2021_Optimal Transport Kernels for Sequential and Parallel Neural Architecture Search.pdf},
}

@inproceedings{wan_think_2021,
	title = {Think {Global} and {Act} {Local}: {Bayesian} {Optimisation} over {High}-{Dimensional} {Categorical} and {Mixed} {Search} {Spaces}},
	shorttitle = {Think {Global} and {Act} {Local}},
	url = {http://arxiv.org/abs/2102.07188},
	abstract = {High-dimensional black-box optimisation remains an important yet notoriously challenging problem. Despite the success of Bayesian optimisation methods on continuous domains, domains that are categorical, or that mix continuous and categorical variables, remain challenging. We propose a novel solution -- we combine local optimisation with a tailored kernel design, effectively handling high-dimensional categorical and mixed search spaces, whilst retaining sample efficiency. We further derive convergence guarantee for the proposed approach. Finally, we demonstrate empirically that our method outperforms the current baselines on a variety of synthetic and real-world tasks in terms of performance, computational costs, or both.},
	urldate = {2021-08-11},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	author = {Wan, Xingchen and Nguyen, Vu and Ha, Huong and Ru, Binxin and Lu, Cong and Osborne, Michael A.},
	month = jun,
	year = {2021},
	note = {https://github.com/xingchenwan/Casmopolitan},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Wan et al_2021_Think Global and Act Local.pdf:/Users/mosb/Google Drive/Zotero/Wan et al_2021_Think Global and Act Local.pdf},
}

@inproceedings{ru_bayesian_2020,
	title = {Bayesian {Optimisation} over {Multiple} {Continuous} and {Categorical} {Inputs}},
	url = {http://proceedings.mlr.press/v119/ru20a.html},
	abstract = {Efficient optimisation of black-box problems that comprise both continuous and categorical inputs is important, yet poses significant challenges. We propose a new approach, Continuous and Categorical Bayesian Optimisation (CoCaBO), which combines the strengths of multi-armed bandits and Bayesian optimisation to select values for both categorical and continuous inputs. We model this mixed-type space using a Gaussian Process kernel, designed to allow sharing of information across multiple categorical variables, each with multiple possible values; this allows CoCaBO to leverage all available data efficiently. We extend our method to the batch setting and propose an efficient selection procedure that dynamically balances exploration and exploitation whilst encouraging batch diversity. We demonstrate empirically that our method outperforms existing approaches on both synthetic and real-world optimisation tasks with continuous and categorical inputs.},
	language = {en},
	urldate = {2021-08-11},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {PMLR},
	author = {Ru, Binxin and Alvi, Ahsan and Nguyen, Vu and Osborne, Michael A. and Roberts, Stephen},
	month = nov,
	year = {2020},
	note = {https://github.com/rubinxin/CoCaBO\_code},
	pages = {8276--8285},
	file = {Ru et al_2020_Bayesian Optimisation over Multiple Continuous and Categorical Inputs.pdf:/Users/mosb/Google Drive/Zotero/Ru et al_2020_Bayesian Optimisation over Multiple Continuous and Categorical Inputs.pdf},
}

@inproceedings{nguyen_knowing_2020,
	title = {Knowing {The} {What} {But} {Not} {The} {Where} in {Bayesian} {Optimization}},
	url = {http://proceedings.mlr.press/v119/nguyen20d.html},
	abstract = {Bayesian optimization has demonstrated impressive success in finding the optimum location x* and value f*=f(x*)=max f(x) of the black-box function f. In some applications, however, the optimum value is known in advance and the goal is to find the corresponding optimum location. Existing work in Bayesian optimization (BO) has not effectively exploited the knowledge of f* for optimization. In this paper, we consider a new setting in BO in which the knowledge of the optimum value is available. Our goal is to exploit the knowledge about f* to search for the location x* efficiently. To achieve this goal, we first transform the Gaussian process surrogate using the information about the optimum value. Then, we propose two acquisition functions, called confidence bound minimization and expected regret minimization, which exploit the knowledge about the optimum value to identify the optimum location efficiently. We show that our approaches work both intuitively and quantitatively achieve better performance against standard BO methods. We demonstrate real applications in tuning a deep reinforcement learning algorithm on the CartPole problem and XGBoost on Skin Segmentation dataset in which the optimum values are publicly available.},
	language = {en},
	urldate = {2021-08-11},
	booktitle = {International {Conference} on {Machine} {Learning} ({ICML})},
	publisher = {PMLR},
	author = {Nguyen, Vu and Osborne, Michael A.},
	month = nov,
	year = {2020},
	note = {https://github.com/ntienvu/KnownOptimum\_BO},
	pages = {7317--7326},
	file = {Nguyen_Osborne_2020_Knowing The What But Not The Where in Bayesian Optimization.pdf:/Users/mosb/Google Drive/Zotero/Nguyen_Osborne_2020_Knowing The What But Not The Where in Bayesian Optimization.pdf},
}

@inproceedings{nguyen_gaussian_2020,
	title = {Gaussian {Process} {Bandit} {Optimization} of the {Thermodynamic} {Variational} {Objective}},
	url = {http://arxiv.org/abs/2010.15750},
	abstract = {Achieving the full promise of the Thermodynamic Variational Objective (TVO), a recently proposed variational lower bound on the log evidence involving a one-dimensional Riemann integral approximation, requires choosing a "schedule" of sorted discretization points. This paper introduces a bespoke Gaussian process bandit optimization method for automatically choosing these points. Our approach not only automates their one-time selection, but also dynamically adapts their positions over the course of optimization, leading to improved model learning and inference. We provide theoretical guarantees that our bandit optimization converges to the regret-minimizing choice of integration points. Empirical validation of our algorithm is provided in terms of improved learning and inference in Variational Autoencoders and Sigmoid Belief Networks.},
	urldate = {2021-08-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Nguyen, Vu and Masrani, Vaden and Brekelmans, Rob and Osborne, Michael A. and Wood, Frank},
	month = nov,
	year = {2020},
	note = {https://github.com/ntienvu/tvo\_gp\_bandit},
	keywords = {Computer Science - Machine Learning},
	file = {Nguyen et al_2020_Gaussian Process Bandit Optimization of the Thermodynamic Variational Objective.pdf:/Users/mosb/Google Drive/Zotero/Nguyen et al_2020_Gaussian Process Bandit Optimization of the Thermodynamic Variational Objective.pdf},
}

@inproceedings{ru_interpretable_2021,
	title = {Interpretable {Neural} {Architecture} {Search} via {Bayesian} {Optimisation} with {Weisfeiler}-{Lehman} {Kernels}},
	url = {http://arxiv.org/abs/2006.07556},
	abstract = {Current neural architecture search (NAS) strategies focus only on finding a single, good, architecture. They offer little insight into why a specific network is performing well, or how we should modify the architecture if we want further improvements. We propose a Bayesian optimisation (BO) approach for NAS that combines the Weisfeiler-Lehman graph kernel with a Gaussian process surrogate. Our method optimises the architecture in a highly data-efficient manner: it is capable of capturing the topological structures of the architectures and is scalable to large graphs, thus making the high-dimensional and graph-like search spaces amenable to BO. More importantly, our method affords interpretability by discovering useful network features and their corresponding impact on the network performance. Indeed, we demonstrate empirically that our surrogate model is capable of identifying useful motifs which can guide the generation of new architectures. We finally show that our method outperforms existing NAS approaches to achieve the state of the art on both closed- and open-domain search spaces.},
	urldate = {2021-08-11},
	booktitle = {International {Conference} on {Learning} {Representations} ({ICLR})},
	author = {Ru, Binxin and Wan, Xingchen and Dong, Xiaowen and Osborne, Michael},
	month = feb,
	year = {2021},
	note = {https://github.com/xingchenwan/nasbowl},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Ru et al_2021_Interpretable Neural Architecture Search via Bayesian Optimisation with.pdf:/Users/mosb/Google Drive/Zotero/Ru et al_2021_Interpretable Neural Architecture Search via Bayesian Optimisation with.pdf},
}

@inproceedings{nguyen_bayesian_2020,
	title = {Bayesian {Optimization} for {Iterative} {Learning}},
	url = {http://arxiv.org/abs/1909.09593},
	abstract = {The performance of deep (reinforcement) learning systems crucially depends on the choice of hyperparameters. Their tuning is notoriously expensive, typically requiring an iterative training process to run for numerous steps to convergence. Traditional tuning algorithms only consider the final performance of hyperparameters acquired after many expensive iterations and ignore intermediate information from earlier training steps. In this paper, we present a Bayesian optimization (BO) approach which exploits the iterative structure of learning algorithms for efficient hyperparameter tuning. We propose to learn an evaluation function compressing learning progress at any stage of the training process into a single numeric score according to both training success and stability. Our BO framework is then balancing the benefit of assessing a hyperparameter setting over additional training steps against their computation cost. We further increase model efficiency by selectively including scores from different training steps for any evaluated hyperparameter set. We demonstrate the efficiency of our algorithm by tuning hyperparameters for the training of deep reinforcement learning agents and convolutional neural networks. Our algorithm outperforms all existing baselines in identifying optimal hyperparameters in minimal time.},
	urldate = {2021-08-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} ({NeurIPS})},
	author = {Nguyen, Vu and Schulze, Sebastian and Osborne, Michael A.},
	year = {2020},
	note = {https://github.com/ntienvu/KnownOptimum\_BO},
	keywords = {Computer Science - Machine Learning, preprint, Statistics - Machine Learning},
	file = {Nguyen et al_2021_Bayesian Optimization for Iterative Learning.pdf:/Users/mosb/Google Drive/Zotero/Nguyen et al_2021_Bayesian Optimization for Iterative Learning.pdf},
}

@inproceedings{adachi_looping_2024,
	title = {Looping in the {Human}: {Collaborative} and {Explainable} {Bayesian} {Optimization}},
	shorttitle = {Looping in the {Human}},
	url = {https://proceedings.mlr.press/v238/adachi24a.html},
	abstract = {Like many optimizers, Bayesian optimization often falls short of gaining user trust due to opacity. While attempts have been made to develop human-centric optimizers, they typically assume user knowledge is well-specified and error-free, employing users mainly as supervisors of the optimization process. We relax these assumptions and propose a more balanced human-AI partnership with our Collaborative and Explainable Bayesian Optimization (CoExBO) framework. Instead of explicitly requiring a user to provide a knowledge model, CoExBO employs preference learning to seamlessly integrate human insights into the optimization, resulting in algorithmic suggestions that resonate with user preference. CoExBO explains its candidate selection every iteration to foster trust, empowering users with a clearer grasp of the optimization. Furthermore, CoExBO offers a no-harm guarantee, allowing users to make mistakes; even with extreme adversarial interventions, the algorithm converges asymptotically to a vanilla Bayesian optimization. We validate CoExBO’s efficacy through human-AI teaming experiments in lithium-ion battery design, highlighting substantial improvements over conventional methods. Code is available https://github.com/ma921/CoExBO.},
	language = {en},
	urldate = {2024-05-23},
	booktitle = {Proceedings of {The} 27th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Adachi, Masaki and Planden, Brady and Howey, David and Osborne, Michael A. and Orbell, Sebastian and Ares, Natalia and Muandet, Krikamol and Chau, Siu Lun},
	month = apr,
	year = {2024},
	note = {https://github.com/ma921/CoExBO},
	pages = {505--513},
	file = {6657/Adachi et al. - 2024 - Looping in the Human Collaborative and Explainabl.pdf},
}

@inproceedings{adachi_adaptive_2024,
	title = {Adaptive {Batch} {Sizes} for {Active} {Learning}: {A} {Probabilistic} {Numerics} {Approach}},
	shorttitle = {Adaptive {Batch} {Sizes} for {Active} {Learning}},
	url = {https://proceedings.mlr.press/v238/adachi24b.html},
	abstract = {Active learning parallelization is widely used, but typically relies on fixing the batch size throughout experimentation. This fixed approach is inefficient because of a dynamic trade-off between cost and speed—larger batches are more costly, smaller batches lead to slower wall-clock run-times—and the trade-off may change over the run (larger batches are often preferable earlier). To address this trade-off, we propose a novel Probabilistic Numerics framework that adaptively changes batch sizes. By framing batch selection as a quadrature task, our integration-error-aware algorithm facilitates the automatic tuning of batch sizes to meet predefined quadrature precision objectives, akin to how typical optimizers terminate based on convergence thresholds. This approach obviates the necessity for exhaustive searches across all potential batch sizes. We also extend this to scenarios with constrained active learning and constrained optimization, interpreting constraint violations as reductions in the precision requirement, to subsequently adapt batch construction. Through extensive experiments, we demonstrate that our approach significantly enhances learning efficiency and flexibility in diverse Bayesian batch active learning and Bayesian optimization applications.},
	language = {en},
	urldate = {2024-05-23},
	booktitle = {Proceedings of {The} 27th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Adachi, Masaki and Hayakawa, Satoshi and Jørgensen, Martin and Wan, Xingchen and Nguyen, Vu and Oberhauser, Harald and Osborne, Michael A.},
	month = apr,
	year = {2024},
	note = {https://github.com/ma921/AdaBatAL},
	pages = {496--504},
	file = {6659/Adachi et al. - 2024 - Adaptive Batch Sizes for Active Learning A Probab.pdf},
}

@article{wan_bayesian_2023,
	title = {Bayesian {Optimisation} of {Functions} on {Graphs}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/86419aba4e5eafd2b1009a2e3c540bb0-Abstract-Conference.html},
	abstract = {The increasing availability of graph-structured data motivates the task of optimising over functions defined on the node set of graphs. Traditional graph search algorithms can be applied in this case, but they may be sample-inefficient and do not make use of information about the function values; on the other hand, Bayesian optimisation is a class of promising black-box solvers with superior sample efficiency, but it has scarcely been applied to such novel setups. To fill this gap, we propose a novel Bayesian optimisation framework that optimises over functions defined on generic, large-scale and potentially unknown graphs. Through the learning of suitable kernels on graphs, our framework has the advantage of adapting to the behaviour of the target function. The local modelling approach further guarantees the efficiency of our method. Extensive experiments on both synthetic and real-world graphs demonstrate the effectiveness of the proposed optimisation framework.},
	language = {en},
	urldate = {2024-05-23},
	journal = {Advances in Neural Information Processing Systems},
	author = {Wan, Xingchen and Osselin, Pierre and Kenlay, Henry and Ru, Binxin and Osborne, Michael A. and Dong, Xiaowen},
	month = dec,
	year = {2023},
	pages = {43012--43040},
	file = {6665/Wan et al. - 2023 - Bayesian Optimisation of Functions on Graphs.pdf},
}

@article{adachi_fast_2022,
	title = {Fast {Bayesian} {Inference} with {Batch} {Bayesian} {Quadrature} via {Kernel} {Recombination}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/697200c9d1710c2799720b660abd11bb-Abstract-Conference.html},
	abstract = {Calculation of Bayesian posteriors and model evidences typically requires numerical integration. Bayesian quadrature (BQ), a surrogate-model-based approach to numerical integration, is capable of superb sample efficiency, but its lack of parallelisation has hindered its practical applications. In this work, we propose a parallelised (batch) BQ method, employing techniques from kernel quadrature, that possesses an empirically exponential convergence rate.Additionally, just as with Nested Sampling, our method permits simultaneous inference of both posteriors and model evidence.Samples from our BQ surrogate model are re-selected to give a sparse set of samples, via a kernel recombination algorithm, requiring negligible additional time to increase the batch size.Empirically, we find that our approach significantly outperforms the sampling efficiency of both state-of-the-art BQ techniques and Nested Sampling in various real-world datasets, including lithium-ion battery analytics.},
	language = {en},
	urldate = {2024-05-23},
	journal = {Advances in Neural Information Processing Systems},
	author = {Adachi, Masaki and Hayakawa, Satoshi and Jørgensen, Martin and Oberhauser, Harald and Osborne, Michael A.},
	month = dec,
	year = {2022},
	note = {https://github.com/ma921/BASQ},
	pages = {16533--16547},
	file = {6680/Adachi et al. - 2022 - Fast Bayesian Inference with Batch Bayesian Quadra.pdf},
}

@article{adachi_bayesian_2023,
	series = {22nd {IFAC} {World} {Congress}},
	title = {Bayesian {Model} {Selection} of {Lithium}-{Ion} {Battery} {Models} via {Bayesian} {Quadrature}},
	volume = {56},
	issn = {2405-8963},
	url = {https://doi.org/10.1016/j.ifacol.2023.10.1073},
	doi = {10.1016/j.ifacol.2023.10.1073},
	abstract = {A wide variety of battery models are available, and it is not always obvious which model ‘best’ describes a dataset. This paper presents a Bayesian model selection approach using Bayesian quadrature. The model evidence is adopted as the selection metric, choosing the simplest model that describes the data, in the spirit of Occam's razor. However, estimating this requires integral computations over parameter space, which is usually prohibitively expensive. Bayesian quadrature offers sample-efficient integration via model-based inference that minimises the number of battery model evaluations. The posterior distribution of model parameters can also be inferred as a byproduct without further computation. Here, the simplest lithium-ion battery models, equivalent circuit models, were used to analyse the sensitivity of the selection criterion to given different datasets and model configurations. We show that popular model selection criteria, such as root-mean-square error and Bayesian information criterion, can fail to select a parsimonious model in the case of a multimodal posterior. The model evidence can spot the optimal model in such cases, simultaneously providing the variance of the evidence inference itself as an indication of confidence. We also show that Bayesian quadrature can compute the evidence faster than popular Monte Carlo based solvers.},
	number = {2},
	urldate = {2024-05-23},
	journal = {IFAC-PapersOnLine},
	author = {Adachi, Masaki and Kuhn, Yannick and Horstmann, Birger and Latz, Arnulf and Osborne, Michael A. and Howey, David A.},
	month = jan,
	year = {2023},
	note = {https://github.com/Battery-Intelligence-Lab/BayesianModelSelection},
	keywords = {battery, Bayesian, estimation, identifiability, lithium-ion, system identification},
	pages = {10521--10526},
	file = {6686/S2405896323014763.html:text/html;Submitted Version:files/6687/Adachi et al. - 2023 - Bayesian Model Selection of Lithium-Ion Battery Mo.pdf},
}

@article{cohen_log-linear-time_2022,
	title = {Log-{Linear}-{Time} {Gaussian} {Processes} {Using} {Binary} {Tree} {Kernels}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/359ddb9caccb4c54cc915dceeacf4892-Abstract-Conference.html},
	abstract = {Gaussian processes (GPs) produce good probabilistic models of functions, but most GP kernels require O((n+m)n2) time, where n is the number of data points and m the number of predictive locations. We present a new kernel that allows for Gaussian process regression in O((n+m)log(n+m)) time. Our "binary tree" kernel places all data points on the leaves of a binary tree, with the kernel depending only on the depth of the deepest common ancestor. We can store the resulting kernel matrix in O(n) space in O(nlogn) time, as a sum of sparse rank-one matrices, and approximately invert the kernel matrix in O(n) time. Sparse GP methods also offer linear run time, but they predict less well than higher dimensional kernels. On a classic suite of regression tasks, we compare our kernel against Mat{\textbackslash}'ern, sparse, and sparse variational kernels. The binary tree GP assigns the highest likelihood to the test data on a plurality of datasets, usually achieves lower mean squared error than the sparse methods, and often ties or beats the Mat{\textbackslash}'ern GP. On large datasets, the binary tree GP is fastest, and much faster than a Mat{\textbackslash}'ern GP.},
	language = {en},
	urldate = {2024-05-24},
	journal = {Advances in Neural Information Processing Systems},
	author = {Cohen, Michael K. and Daulton, Samuel and Osborne, Michael A.},
	month = dec,
	year = {2022},
	note = {https://colab.research.google.com/drive/1Fcr9Tne40fV6HjZaqfXducl00L\_MzUX8?usp=sharing},
	pages = {8118--8129},
	file = {6700/Cohen et al. - 2022 - Log-Linear-Time Gaussian Processes Using Binary Tr.pdf},
}

@article{jorgensen_bezier_2022,
	title = {Bezier {Gaussian} {Processes} for {Tall} and {Wide} {Data}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/99c80ceb10cb674110f03b2def6a5b76-Abstract-Conference.html},
	abstract = {Modern approximations to Gaussian processes are suitable for tall data'', with a cost that scales well in the number of observations, but under-performs onwide data'', scaling poorly in the number of input features. That is, as the number of input features grows, good predictive performance requires the number of summarising variables, and their associated cost, to grow rapidly. We introduce a kernel that allows the number of summarising variables to grow exponentially with the number of input features, but requires only linear cost in both number of observations and input features. This scaling is achieved through our introduction of the Bezier buttress'', which allows approximate inference without computing matrix inverses or determinants. We show that our kernel has close similarities to some of the most used kernels in Gaussian process regression, and empirically demonstrate the kernel's ability to scale to both tall and wide datasets.},
	language = {en},
	urldate = {2024-05-24},
	journal = {Advances in Neural Information Processing Systems},
	author = {Jørgensen, Martin and Osborne, Michael A.},
	month = dec,
	year = {2022},
	pages = {24354--24366},
	file = {6702/Jørgensen and Osborne - 2022 - Bezier Gaussian Processes for Tall and Wide Data.pdf},
}

@article{daulton_bayesian_2022,
	title = {Bayesian {Optimization} over {Discrete} and {Mixed} {Spaces} via {Probabilistic} {Reparameterization}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/531230cfac80c65017ad0f85d3031edc-Abstract-Conference.html},
	abstract = {Optimizing expensive-to-evaluate black-box functions of discrete (and potentially continuous) design parameters is a ubiquitous problem in scientific and engineering applications. Bayesian optimization (BO) is a popular, sample-efficient method that leverages a probabilistic surrogate model and an acquisition function (AF) to select promising designs to evaluate. However, maximizing the AF over mixed or high-cardinality discrete search spaces is challenging standard gradient-based methods cannot be used directly or evaluating the AF at every point in the search space would be computationally prohibitive. To address this issue, we propose using probabilistic reparameterization (PR). Instead of directly optimizing the AF over the search space containing discrete parameters, we instead maximize the expectation of the AF over a probability distribution defined by continuous parameters. We prove that under suitable reparameterizations, the BO policy that maximizes the probabilistic objective is the same as that which maximizes the AF, and therefore, PR enjoys the same regret bounds as the original BO policy using the underlying AF. Moreover, our approach provably converges to a stationary point of the probabilistic objective under gradient ascent using scalable, unbiased estimators of both the probabilistic objective and its gradient. Therefore, as the number of starting points and gradient steps increase, our approach will recover of a maximizer of the AF (an often-neglected requisite for commonly used BO regret bounds). We validate our approach empirically and demonstrate state-of-the-art optimization performance on a wide range of real-world applications. PR is complementary to (and benefits) recent work and naturally generalizes to settings with multiple objectives and black-box constraints.},
	language = {en},
	urldate = {2024-05-24},
	journal = {Advances in Neural Information Processing Systems},
	author = {Daulton, Samuel and Wan, Xingchen and Eriksson, David and Balandat, Maximilian and Osborne, Michael A. and Bakshy, Eytan},
	month = dec,
	year = {2022},
	note = {https://github.com/facebookresearch/bo\_pr},
	pages = {12760--12774},
	file = {6704/Daulton et al. - 2022 - Bayesian Optimization over Discrete and Mixed Spac.pdf},
}

@inproceedings{wan_bayesian_2022,
	title = {Bayesian {Generational} {Population}-{Based} {Training}},
	url = {https://proceedings.mlr.press/v188/wan22a.html},
	abstract = {Reinforcement learning (RL) offers the potential for training generally capable agents that can interact autonomously in the real world. However, one key limitation is the brittleness of RL algorithms to core hyperparameters and network architecture choice. Furthermore, non-stationarities such as evolving training data and increased agent complexity mean that different hyperparameters and architectures may be optimal at different points of training. This motivates AutoRL, a class of methods seeking to automate these design choices. One prominent class of AutoRL methods is Population-Based Training (PBT), which have led to impressive performance in several large scale settings. In this paper, we introduce two new innovations in PBT-style methods. First, we employ trust-region based Bayesian Optimization, enabling full coverage of the high-dimensional mixed hyperparameter search space. Second, we show that using a generational approach, we can also learn both architectures and hyperparameters jointly on-the-fly in a single training run. Leveraging the new highly parallelizable Brax physics engine, we show that these innovations lead to dramatic performance gains, significantly outperforming the tuned baseline while learning entire configurations on the fly.},
	language = {en},
	urldate = {2024-05-24},
	booktitle = {Proceedings of the {First} {International} {Conference} on {Automated} {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wan, Xingchen and Lu, Cong and Parker-Holder, Jack and Ball, Philip J. and Nguyen, Vu and Ru, Binxin and Osborne, Michael},
	month = sep,
	year = {2022},
	note = {https://github.com/xingchenwan/bgpbt},
	pages = {14/1--27},
	file = {6706/Wan et al. - 2022 - Bayesian Generational Population-Based Training.pdf},
}

@inproceedings{daulton_robust_2022,
	title = {Robust {Multi}-{Objective} {Bayesian} {Optimization} {Under} {Input} {Noise}},
	url = {https://proceedings.mlr.press/v162/daulton22a.html},
	abstract = {Bayesian optimization (BO) is a sample-efficient approach for tuning design parameters to optimize expensive-to-evaluate, black-box performance metrics. In many manufacturing processes, the design parameters are subject to random input noise, resulting in a product that is often less performant than expected. Although BO methods have been proposed for optimizing a single objective under input noise, no existing method addresses the practical scenario where there are multiple objectives that are sensitive to input perturbations. In this work, we propose the first multi-objective BO method that is robust to input noise. We formalize our goal as optimizing the multivariate value-at-risk (MVaR), a risk measure of the uncertain objectives. Since directly optimizing MVaR is computationally infeasible in many settings, we propose a scalable, theoretically-grounded approach for optimizing MVaR using random scalarizations. Empirically, we find that our approach significantly outperforms alternative methods and efficiently identifies optimal robust designs that will satisfy specifications across multiple metrics with high probability.},
	language = {en},
	urldate = {2024-05-24},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Daulton, Samuel and Cakmak, Sait and Balandat, Maximilian and Osborne, Michael A. and Zhou, Enlu and Bakshy, Eytan},
	month = jun,
	year = {2022},
	note = {https://github.com/facebookresearch/robust\_mobo},
	pages = {4831--4866},
	file = {6710/Daulton et al. - 2022 - Robust Multi-Objective Bayesian Optimization Under.pdf},
}

@inproceedings{wan_adversarial_2021,
	title = {Adversarial {Attacks} on {Graph} {Classifiers} via {Bayesian} {Optimisation}},
	volume = {34},
	url = {https://papers.nips.cc/paper_files/paper/2021/hash/38811c5285e34e2e3319ab7d9f2cfa5b-Abstract.html},
	abstract = {Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks. While the majority of the literature focuses on such vulnerability in node-level classification tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classification, an important problem with numerous real-life applications such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classification models. Our method is black-box, query-efficient and parsimonious with respect to the perturbation applied. We empirically validate the effectiveness and flexibility of the proposed method on a wide range of graph classification tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adversarial robustness of graph classification models.},
	urldate = {2024-05-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wan, Xingchen and Kenlay, Henry and Ru, Robin and Blaas, Arno and Osborne, Michael A and Dong, Xiaowen},
	year = {2021},
	note = {https://github.com/xingchenwan/grabnel},
	pages = {6983--6996},
	file = {6719/Wan et al. - 2021 - Adversarial Attacks on Graph Classifiers via Bayes.pdf},
}

@inproceedings{rudner_pathologies_2021,
	title = {On {Pathologies} in {KL}-{Regularized} {Reinforcement} {Learning} from {Expert} {Demonstrations}},
	volume = {34},
	url = {https://papers.nips.cc/paper_files/paper/2021/hash/eecca5b6365d9607ee5a9d336962c534-Abstract.html},
	abstract = {KL-regularized reinforcement learning from expert demonstrations has proved successful in improving the sample efficiency of deep reinforcement learning algorithms, allowing them to be applied to challenging physical real-world tasks. However, we show that KL-regularized reinforcement learning with behavioral reference policies derived from expert demonstrations can suffer from pathological training dynamics that can lead to slow, unstable, and suboptimal online learning. We show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efficiency and online policy performance. Finally, we show that the pathology can be remedied by non-parametric behavioral reference policies and that this allows KL-regularized reinforcement learning to significantly outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks.},
	urldate = {2024-05-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rudner, Tim G. J. and Lu, Cong and Osborne, Michael A and Gal, Yarin and Teh, Yee},
	year = {2021},
	note = {https://github.com/conglu1997/nppac},
	pages = {28376--28389},
	file = {6721/Rudner et al. - 2021 - On Pathologies in KL-Regularized Reinforcement Lea.pdf},
}

@inproceedings{lu_revisiting_2021,
	title = {Revisiting {Design} {Choices} in {Offline} {Model} {Based} {Reinforcement} {Learning}},
	url = {https://openreview.net/forum?id=zz9hXVhf40},
	abstract = {Offline reinforcement learning enables agents to leverage large pre-collected datasets of environment transitions to learn control policies, circumventing the need for potentially expensive or unsafe online data collection. Significant progress has been made recently in offline model-based reinforcement learning, approaches which leverage a learned dynamics model. This typically involves constructing a probabilistic model, and using the model uncertainty to penalize rewards where there is insufficient data, solving for a pessimistic MDP that lower bounds the true MDP. Existing methods, however, exhibit a breakdown between theory and practice, whereby pessimistic return ought to be bounded by the total variation distance of the model from the true dynamics, but is instead implemented through a penalty based on estimated model uncertainty. This has spawned a variety of uncertainty heuristics, with little to no comparison between differing approaches. In this paper, we compare these heuristics, and design novel protocols to investigate their interaction with other hyperparameters, such as the number of models, or imaginary rollout horizon. Using these insights, we show that selecting these key hyperparameters using Bayesian Optimization produces superior configurations that are vastly different to those currently used in existing hand-tuned state-of-the-art methods, and result in drastically stronger performance.},
	language = {en},
	urldate = {2024-05-24},
	author = {Lu, Cong and Ball, Philip and Parker-Holder, Jack and Osborne, Michael and Roberts, Stephen J.},
	month = oct,
	year = {2021},
	file = {6727/Lu et al. - 2021 - Revisiting Design Choices in Offline Model Based R.pdf},
}

@inproceedings{nguyen_optimal_2021-1,
	title = {Optimal {Transport} {Kernels} for {Sequential} and {Parallel} {Neural} {Architecture} {Search}},
	url = {https://proceedings.mlr.press/v139/nguyen21d.html},
	abstract = {Neural architecture search (NAS) automates the design of deep neural networks. One of the main challenges in searching complex and non-continuous architectures is to compare the similarity of networks that the conventional Euclidean metric may fail to capture. Optimal transport (OT) is resilient to such complex structure by considering the minimal cost for transporting a network into another. However, the OT is generally not negative definite which may limit its ability to build the positive-definite kernels required in many kernel-dependent frameworks. Building upon tree-Wasserstein (TW), which is a negative definite variant of OT, we develop a novel discrepancy for neural architectures, and demonstrate it within a Gaussian process surrogate model for the sequential NAS settings. Furthermore, we derive a novel parallel NAS, using quality k-determinantal point process on the GP posterior, to select diverse and high-performing architectures from a discrete set of candidates. Empirically, we demonstrate that our TW-based approaches outperform other baselines in both sequential and parallel NAS.},
	language = {en},
	urldate = {2024-05-24},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nguyen, Vu and Le, Tam and Yamada, Makoto and Osborne, Michael A.},
	month = jul,
	year = {2021},
	pages = {8084--8095},
	file = {6734/Nguyen et al. - 2021 - Optimal Transport Kernels for Sequential and Paral.pdf},
}
