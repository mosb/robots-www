
@article{nickson_blitzkriging_2015,
	title = {Blitzkriging : {Kronecker}-structured {Stochastic} {Gaussian} {Processes}},
	shorttitle = {Blitzkriging},
	url = {http://arxiv.org/abs/1510.07965},
	abstract = {We present Blitzkriging, a new approach to fast inference for Gaussian processes, applicable to regression, optimisation and classification. State-of-the-art (stochastic) inference for Gaussian processes on very large datasets scales cubically in the number of 'inducing inputs', variables introduced to factorise the model. Blitzkriging shares state-of-the-art scaling with data, but reduces the scaling in the number of inducing points to approximately linear. Further, in contrast to other methods, Blitzkriging: does not force the data to conform to any particular structure (including grid-like); reduces reliance on error-prone optimisation of inducing point locations; and is able to learn rich (covariance) structure from the data. We demonstrate the benefits of our approach on real data in regression, time-series prediction and signal-interpolation experiments.},
	journal = {arXiv:1510.07965 [stat]},
	author = {Nickson, Thomas and Gunter, Tom and Lloyd, Chris and Osborne, Michael A. and Roberts, Stephen},
	month = oct,
	year = {2015},
	keywords = {preprint},
	file = {1299/Nickson et al. - 2015 - Blitzkriging Kronecker-structured Stochastic Gaus.pdf}
}

@techreport{reece_anomaly_2009,
	title = {Anomaly detection and removal using nonstationary {Gaussian} processes},
	abstract = {This paper proposes a novel Gaussian process approach to
fault removal in time-series data. Fault removal does not delete
the faulty signal data but, instead, massages the fault from
the data. We assume that only one fault occurs at any one
time and model the signal by two separate non-parametric
Gaussian process models for both the physical phenomenon
and the fault. In order to facilitate fault removal we introduce
the Markov Region Link kernel for handling non-stationary
Gaussian Processes. This kernel is piece-wise stationary but
guarantees that functions generated by it and their derivatives
(when required) are everywhere continuous. We apply this
kernel to the removal of drift and bias errors in faulty sensor
data and also to the recovery of EOG artifact corrupted EEG
signals.},
	institution = {Department of Engineering Science, University of Oxford},
	author = {Reece, Steve and Garnett, Roman and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2009},
	keywords = {preprint},
	file = {1108/reece2009.pdf}
}

@article{gillani_communication_2014,
	title = {Communication {Communities} in {MOOCs}},
	abstract = {Massive Open Online Courses (MOOCs) bring together thousands of people from different geographies and demographic backgrounds -- but to date, little is known about how they learn or communicate. We introduce a new content-analysed MOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to extract communities of learners based on the nature of their online forum posts. We see that BNMF yields a superior probabilistic generative model for online discussions when compared to other models, and that the communities it learns are differentiated by their composite students' demographic and course performance indicators. These findings suggest that computationally efficient probabilistic generative modelling of MOOCs can reveal important insights for educational researchers and practitioners and help to develop more intelligent and responsive online learning environments.},
	journal = {arXiv preprint arXiv:1403.4640},
	author = {Gillani, Nabeel and Eynon, Rebecca and Osborne, Michael A. and Hjorth, Isis and Roberts, Stephen},
	year = {2014},
	keywords = {preprint},
	file = {1015/1403.4640.pdf}
}

@misc{osborne_epistemic_2008,
	title = {Epistemic {Uncertainty} in {Quantum} {Mechanics}},
	author = {Osborne, Michael A.},
	month = apr,
	year = {2008},
	keywords = {preprint},
	file = {1460/Osborne2008Epistemic_uncertainty_in_quantum_mechanics.pdf}
}

@article{hutter_kernel_2013,
	title = {A {Kernel} for {Hierarchical} {Parameter} {Spaces}},
	url = {http://arxiv.org/abs/1310.5738},
	abstract = {We define a family of kernels for mixed continuous/discrete hierarchical parameter spaces and show that they are positive definite.},
	journal = {arXiv preprint arXiv:1310.5738},
	author = {Hutter, Frank and Osborne, Michael A.},
	year = {2013},
	keywords = {preprint},
	file = {2152/1310.5738v1.pdf}
}

@inproceedings{swersky_raiders_2013,
	title = {Raiders of the lost architecture\_ {Kernels} for {Bayesian} optimization in conditional parameter spaces},
	abstract = {In practical Bayesian optimization, we must often search over structures with differing numbers of parameters. For instance, we may wish to search over neural network architectures with an unknown number of layers. To relate performance data gathered for different architectures, we define a new kernel for conditional parameter spaces that explicitly includes information about which parameters are relevant in a given structure. We show that this kernel improves model quality and Bayesian optimization results over several simpler baseline kernels.},
	booktitle = {{NIPS} workshop on {Bayesian} {Optimization} in theory and practice ({BayesOpt}’13)},
	author = {Swersky, Kevin and Duvenaud, David and Snoek, Jasper and Hutter, Frank and Osborne, Michael A.},
	year = {2013},
	keywords = {preprint},
	file = {1173/hier-kern-workshop.pdf}
}

@techreport{osborne_gaussian_2007,
	title = {Gaussian processes for prediction},
	abstract = {We propose a powerful prediction algorithm built upon Gaussian processes (GPs). They are
particularly useful for their flexibility, facilitating accurate prediction even in the absence of strong
physical models.
GPs further allow us to work within a complete Bayesian probabilistic framework. As such, we
show how the hyperparameters of our system can be marginalised by use of Bayesian Monte Carlo, a
principled method of approximate integration. We employ the error bars of our GP’s predictions as
a means to select only the most informative data to store. This allows us to introduce an iterative
formulation of the GP to give a dynamic, on-line algorithm. We also show how our error bars can be
used to perform active data selection, allowing the GP to select where and when it should next take a
measurement.
We demonstrate how our methods can be applied to multi-sensor prediction problems where data
may be missing, delayed and/or correlated. In particular, we present a real network of weather sensors
as a testbed for our algorithm.},
	number = {PARG-07-01},
	institution = {Department of Engineering Science, University of Oxford},
	author = {Osborne, Michael A. and Roberts, Stephen J.},
	year = {2007},
	keywords = {preprint},
	file = {1133/PARG-07-01.pdf}
}

@article{nickson_automated_2014,
	title = {Automated {Machine} {Learning} on {Big} {Data} using {Stochastic} {Algorithm} {Tuning}},
	url = {http://arxiv.org/abs/1407.7969},
	abstract = {We introduce a means of automating machine learning (ML) for big data tasks, by performing scalable stochastic Bayesian optimisation of ML algorithm parameters and hyper-parameters. More often than not, the critical tuning of ML algorithm parameters has relied on domain expertise from experts, along with laborious hand-tuning, brute search or lengthy sampling runs. Against this background, Bayesian optimisation is finding increasing use in automating parameter tuning, making ML algorithms accessible even to non-experts. However, the state of the art in Bayesian optimisation is incapable of scaling to the large number of evaluations of algorithm performance required to fit realistic models to complex, big data. We here describe a stochastic, sparse, Bayesian optimisation strategy to solve this problem, using many thousands of noisy evaluations of algorithm performance on subsets of data in order to effectively train algorithms for big data. We provide a comprehensive benchmarking of possible sparsification strategies for Bayesian optimisation, concluding that a Nystrom approximation offers the best scaling and performance for real tasks. Our proposed algorithm demonstrates substantial improvement over the state of the art in tuning the parameters of a Gaussian Process time series prediction task on real, big data.},
	journal = {arXiv preprint arXiv:1407.7969},
	author = {Nickson, Thomas and Osborne, Michael A. and Reece, Steven and Roberts, Stephen J.},
	year = {2014},
	note = {https://is.gd/e3JAVg},
	keywords = {preprint},
	file = {1051/1407.7969v1.pdf}
}

@article{salas_variational_2015,
	title = {A {Variational} {Bayesian} {State}-{Space} {Approach} to {Online} {Passive}-{Aggressive} {Regression}},
	url = {http://arxiv.org/abs/1509.02438},
	abstract = {Online Passive-Aggressive (PA) learning is a class of online margin-based algorithms suitable for a wide range of real-time prediction tasks, including classification and regression. PA algorithms are formulated in terms of deterministic point-estimation problems governed by a set of user-defined hyperparameters: the approach fails to capture model/prediction uncertainty and makes their performance highly sensitive to hyperparameter configurations. In this paper, we introduce a novel PA learning framework for regression that overcomes the above limitations. We contribute a Bayesian state-space interpretation of PA regression, along with a novel online variational inference scheme, that not only produces probabilistic predictions, but also offers the benefit of automatic hyperparameter tuning. Experiments with various real-world data sets show that our approach performs significantly better than a more standard, linear Gaussian state-space model.},
	journal = {arXiv:1509.02438 [stat]},
	author = {Salas, Arnold and Roberts, Stephen J. and Osborne, Michael A.},
	month = sep,
	year = {2015},
	keywords = {preprint},
	file = {1838/Salas et al. - 2015 - A Variational Bayesian State-Space Approach to Onl.pdf}
}

@article{rizvi_novel_2017,
	title = {A {Novel} {Approach} to {Forecasting} {Financial} {Volatility} with {Gaussian} {Process} {Envelopes}},
	url = {https://arxiv.org/abs/1705.00891},
	abstract = {In this paper we use Gaussian Process (GP) regression to propose a novel approach for predicting volatility of financial returns by forecasting the envelopes of the time series. We provide a direct comparison of their performance to traditional approaches such as GARCH. We compare the forecasting power of three approaches: GP regression on the absolute and squared returns; regression on the envelope of the returns and the absolute returns; and regression on the envelope of the negative and positive returns separately. We use a maximum a posteriori estimate with a Gaussian prior to determine our hyperparameters. We also test the effect of hyperparameter updating at each forecasting step. We use our approaches to forecast out-of-sample volatility of four currency pairs over a 2 year period, at half-hourly intervals. From three kernels, we select the kernel giving the best performance for our data. We use two published accuracy measures and four statistical loss functions to evaluate the forecasting ability of GARCH vs GPs. In mean squared error the GP's perform 20\% better than a random walk model, and 50\% better than GARCH for the same data.},
	urldate = {2017-06-21},
	journal = {arXiv preprint arXiv:1705.00891},
	author = {Rizvi, Syed Ali Asad and Roberts, Stephen J. and Osborne, Michael A. and Nyikosa, Favour},
	year = {2017},
	keywords = {preprint},
	file = {2301/Rizvi et al. - 2017 - A Novel Approach to Forecasting Financial Volatili.pdf}
}

@article{mcleod_practical_2017,
	title = {Practical {Bayesian} {Optimization} for {Variable} {Cost} {Objectives}},
	url = {http://arxiv.org/abs/1703.04335},
	abstract = {We propose a novel Bayesian Optimization approach for black-box functions with an environmental variable whose value determines the tradeoff between evaluation cost and the fidelity of the evaluations. Further, we use a novel approach to sampling support points, allowing faster construction of the acquisition function. This allows us to achieve optimization with lower overheads than previous approaches and is implemented for a more general class of problem. We show this approach to be effective on synthetic and real world benchmark problems.},
	urldate = {2017-06-30},
	journal = {arXiv:1703.04335 [stat]},
	author = {McLeod, Mark and Osborne, Michael A. and Roberts, Stephen J.},
	month = mar,
	year = {2017},
	note = {https://github.com/markm541374/gpbo},
	keywords = {preprint},
	file = {2401/McLeod et al. - 2017 - Practical Bayesian Optimization for Variable Cost .pdf}
}

@article{rontsis_distributionally_2017,
	title = {Distributionally {Ambiguous} {Optimization} {Techniques} in {Batch} {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1707.04191},
	abstract = {We propose a novel, theoretically-grounded, acquisition function for batch Bayesian optimization informed by insights from distributionally ambiguous optimization. Our acquisition function is a lower bound on the well-known Expected Improvement function -- which requires a multi-dimensional Gaussian Expectation over a piecewise affine function -- and is computed by evaluating instead the best-case expectation over all probability distributions consistent with the same mean and variance as the original Gaussian distribution. Unlike alternative approaches including Expected Improvement, our proposed acquisition function avoids multi-dimensional integrations entirely, and can be computed exactly as the solution of a convex optimization problem in the form of a tractable semidefinite program (SDP). Moreover, we prove that the solution of this SDP also yields exact numerical derivatives, which enable efficient optimization of the acquisition function. Finally, it efficiently handles marginalized posteriors with respect to the Gaussian Process' hyperparameters. We demonstrate superior performance to heuristic alternatives and approximations of the intractable expected improvement, justifying this performance difference based on simple examples that break the assumptions of state-of-the-art methods.},
	urldate = {2017-10-31},
	journal = {arXiv:1707.04191 [stat]},
	author = {Rontsis, Nikitas and Osborne, Michael A. and Goulart, Paul J.},
	month = jul,
	year = {2017},
	note = {https://github.com/oxfordcontrol/Bayesian-Optimization},
	keywords = {Statistics - Machine Learning, preprint},
	file = {2865/Rontsis et al. - 2017 - Distributionally Ambiguous Optimization Techniques.pdf}
}

@article{zhao_quantum_2018,
	title = {Quantum algorithms for training {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1803.10520},
	abstract = {Gaussian processes (GPs) are important models in supervised machine learning. Training in Gaussian processes refers to selecting the covariance functions and the associated parameters in order to improve the outcome of predictions, the core of which amounts to evaluating the logarithm of the marginal likelihood (LML) of a given model. LML gives a concrete measure of the quality of prediction that a GP model is expected to achieve. The classical computation of LML typically carries a polynomial time overhead with respect to the input size. We propose a quantum algorithm that computes the logarithm of the determinant of a Hermitian matrix, which runs in logarithmic time for sparse matrices. This is applied in conjunction with a variant of the quantum linear system algorithm that allows for logarithmic time computation of the form \${\textbackslash}mathbf\{y\}{\textasciicircum}TA{\textasciicircum}\{-1\}{\textbackslash}mathbf\{y\}\$, where \${\textbackslash}mathbf\{y\}\$ is a dense vector and \$A\$ is the covariance matrix. We hence show that quantum computing can be used to estimate the LML of a GP with exponentially improved efficiency under certain conditions.},
	urldate = {2018-03-29},
	journal = {arXiv:1803.10520 [quant-ph, stat]},
	author = {Zhao, Zhikuan and Fitzsimons, Jack K. and Osborne, Michael A. and Roberts, Stephen J. and Fitzsimons, Joseph F.},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Learning, Quantum Physics, Statistics - Machine Learning, preprint},
	file = {2962/Zhao et al. - 2018 - Quantum algorithms for training Gaussian Processes.pdf}
}

@article{nyikosa_bayesian_2018,
	title = {Bayesian {Optimization} for {Dynamic} {Problems}},
	url = {http://arxiv.org/abs/1803.03432},
	abstract = {We propose practical extensions to Bayesian optimization for solving dynamic problems. We model dynamic objective functions using spatiotemporal Gaussian process priors which capture all the instances of the functions over time. Our extensions to Bayesian optimization use the information learnt from this model to guide the tracking of a temporally evolving minimum. By exploiting temporal correlations, the proposed method also determines when to make evaluations, how fast to make those evaluations, and it induces an appropriate budget of steps based on the available information. Lastly, we evaluate our technique on synthetic and real-world problems.},
	urldate = {2018-04-16},
	journal = {arXiv:1803.03432 [stat]},
	author = {Nyikosa, Favour M. and Osborne, Michael A. and Roberts, Stephen J.},
	month = mar,
	year = {2018},
	keywords = {Statistics - Machine Learning, preprint},
	file = {3010/Nyikosa et al. - 2018 - Bayesian Optimization for Dynamic Problems.pdf}
}

@article{granziol_entropic_2018,
	title = {Entropic {Spectral} {Learning} in {Large} {Scale} {Networks}},
	url = {http://arxiv.org/abs/1804.06802},
	abstract = {We present a novel algorithm for learning the spectral density of large scale networks using stochastic trace estimation and the method of maximum entropy. The complexity of the algorithm is linear in the number of non-zero elements of the matrix, offering a computational advantage over other algorithms. We apply our algorithm to the problem of community detection in large networks. We show state-of-the-art performance on both synthetic and real datasets.},
	urldate = {2018-04-19},
	journal = {arXiv:1804.06802 [cs, math, stat]},
	author = {Granziol, Diego and Ru, Binxin and Zohren, Stefan and Dong, Xiaowen and Osborne, Michael and Roberts, Stephen},
	month = apr,
	year = {2018},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, preprint, Computer Science - Information Theory},
	file = {3040/Granziol et al. - 2018 - Entropic Spectral Learning in Large Scale Networks.pdf}
}

@article{paul_contextual_2018,
	title = {Contextual {Policy} {Optimisation}},
	url = {http://arxiv.org/abs/1805.10662},
	abstract = {Policy gradient methods have been successfully applied to a variety of reinforcement learning tasks. However, while learning in a simulator, these methods do not utilise the opportunity to improve learning by adjusting certain environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but that are controllable in a simulator. This can lead to slow learning, or convergence to highly suboptimal policies. In this paper, we present contextual policy optimisation (CPO). The central idea is to use Bayesian optimisation to actively select the distribution of the environment variable that maximises the improvement generated by each iteration of the policy gradient method. To make this Bayesian optimisation practical, we contribute two easy-to-compute low-dimensional fingerprints of the current policy. We apply CPO to a number of continuous control tasks of varying difficulty and show that CPO can efficiently learn policies that are robust to significant rare events, which are unlikely to be observable under random sampling but are key to learning good policies.},
	urldate = {2018-05-29},
	journal = {arXiv:1805.10662 [cs, stat]},
	author = {Paul, Supratik and Osborne, Michael A. and Whiteson, Shimon},
	month = may,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning, preprint},
	file = {3160/Paul et al. - 2018 - Contextual Policy Optimisation.pdf}
}

@article{lennon_efficiently_2018,
	title = {Efficiently measuring a quantum device using machine learning},
	url = {http://arxiv.org/abs/1810.10042},
	abstract = {Scalable quantum technologies will present challenges for characterizing and tuning quantum devices. This is a time-consuming activity, and as the size of quantum systems increases, this task will become intractable without the aid of automation. We present measurements on a quantum dot device performed by a machine learning algorithm. The algorithm selects the most informative measurements to perform next using information theory and a probabilistic deep-generative model, the latter capable of generating multiple full-resolution reconstructions from scattered partial measurements. We demonstrate, for two different measurement configurations, that the algorithm outperforms standard grid scan techniques, reducing the number of measurements required by up to 4 times and the measurement time by 3.7 times. Our contribution goes beyond the use of machine learning for data search and analysis, and instead presents the use of algorithms to automate measurement. This work lays the foundation for automated control of large quantum circuits.},
	urldate = {2018-10-25},
	journal = {arXiv:1810.10042 [cond-mat, physics:quant-ph]},
	author = {Lennon, D. T. and Moon, H. and Camenzind, L. C. and Yu, Liuqi and Zumbühl, D. M. and Briggs, G. A. D. and Osborne, M. A. and Laird, E. A. and Ares, N.},
	month = oct,
	year = {2018},
	keywords = {Quantum Physics, Computer Science - Machine Learning, Condensed Matter - Mesoscale and Nanoscale Physics},
	file = {3919/Lennon et al. - 2018 - Efficiently measuring a quantum device using machi.pdf}
}

@article{paul_fingerprint_2018,
	title = {Fingerprint {Policy} {Optimisation} for {Robust} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1805.10662},
	abstract = {Policy gradient methods have been successfully applied to a variety of reinforcement learning tasks. However, while learning in a simulator, these methods do not utilise the opportunity to improve learning by adjusting certain environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but that are controllable in a simulator. This can lead to slow learning or convergence to highly suboptimal policies if the environment variable has a large impact on the transition dynamics. In this paper, we present fingerprint policy optimisation (FPO) which finds a policy that is optimal in expectation across the distribution of environment variables. The central idea is to use Bayesian optimisation (BO) to actively select the distribution of the environment variable that maximises the improvement generated by each iteration of the policy gradient method. To make this BO practical, we contribute two easy-to-compute low-dimensional fingerprints of the current policy. We apply FPO to a number of continuous control tasks of varying difficulty and show that FPO can efficiently learn policies that are robust to significant rare events, which are unlikely to be observable under random sampling but are key to learning good policies.},
	urldate = {2018-10-25},
	journal = {arXiv:1805.10662 [cs, stat]},
	author = {Paul, Supratik and Osborne, Michael A. and Whiteson, Shimon},
	month = may,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, preprint, Computer Science - Machine Learning},
	file = {3922/Paul et al. - 2018 - Fingerprint Policy Optimisation for Robust Reinfor.pdf}
}

@article{wagstaff_limitations_2019,
	title = {On the {Limitations} of {Representing} {Functions} on {Sets}},
	url = {http://arxiv.org/abs/1901.09006},
	abstract = {Recent work on the representation of functions on sets has considered the use of summation in a latent space to enforce permutation invariance. In particular, it has been conjectured that the dimension of this latent space may remain fixed as the cardinality of the sets under consideration increases. However, we demonstrate that the analysis leading to this conjecture requires mappings which are highly discontinuous and argue that this is only of limited practical use. Motivated by this observation, we prove that an implementation of this model via continuous mappings (as provided by e.g. neural networks or Gaussian processes) actually imposes a constraint on the dimensionality of the latent space. Practical universal function representation for set inputs can only be achieved with a latent dimension at least the size of the maximum number of input elements.},
	urldate = {2019-01-28},
	journal = {arXiv:1901.09006 [cs, stat]},
	author = {Wagstaff, Edward and Fuchs, Fabian B. and Engelcke, Martin and Posner, Ingmar and Osborne, Michael},
	month = jan,
	year = {2019},
	keywords = {preprint},
	file = {4124/Wagstaff et al. - 2019 - On the Limitations of Representing Functions on Se.pdf}
}

@article{alvi_asynchronous_2019,
	title = {Asynchronous {Batch} {Bayesian} {Optimisation} with {Improved} {Local} {Penalisation}},
	url = {http://arxiv.org/abs/1901.10452},
	abstract = {Batch Bayesian optimisation (BO) has been successfully applied to hyperparameter tuning using parallel computing, but it is wasteful of resources: workers that complete jobs ahead of others are left idle. We address this problem by developing an approach, Penalising Locally for Asynchronous Bayesian Optimisation on \$k\$ workers (PLAyBOOK), for asynchronous parallel BO. We demonstrate empirically the efficacy of PLAyBOOK and its variants on synthetic tasks and a real-world problem. We undertake a comparison between synchronous and asynchronous BO, and show that asynchronous BO often outperforms synchronous batch BO in both wall-clock time and number of function evaluations.},
	urldate = {2019-01-30},
	journal = {arXiv:1901.10452 [cs, stat]},
	author = {Alvi, Ahsan S. and Ru, Binxin and Calliess, Jan and Roberts, Stephen J. and Osborne, Michael A.},
	month = jan,
	year = {2019},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, preprint, Computer Science - Machine Learning},
	file = {4158/Alvi et al. - 2019 - Asynchronous Batch Bayesian Optimisation with Impr.pdf}
}

@article{chai_automated_2019,
	title = {Automated {Model} {Selection} with {Bayesian} {Quadrature}},
	url = {http://arxiv.org/abs/1902.09724},
	abstract = {We present a novel technique for tailoring Bayesian quadrature (BQ) to model selection. The state-of-the-art for comparing the evidence of multiple models relies on Monte Carlo methods, which converge slowly and are unreliable for computationally expensive models. Previous research has shown that BQ offers sample efficiency superior to Monte Carlo in computing the evidence of an individual model. However, applying BQ directly to model comparison may waste computation producing an overly-accurate estimate for the evidence of a clearly poor model. We propose an automated and efficient algorithm for computing the most-relevant quantity for model selection: the posterior probability of a model. Our technique maximizes the mutual information between this quantity and observations of the models' likelihoods, yielding efficient acquisition of samples across disparate model spaces when likelihood observations are limited. Our method produces more-accurate model posterior estimates using fewer model likelihood evaluations than standard Bayesian quadrature and Monte Carlo estimators, as we demonstrate on synthetic and real-world examples.},
	urldate = {2019-03-01},
	journal = {arXiv:1902.09724 [cs, stat]},
	author = {Chai, Henry and Ton, Jean-Francois and Garnett, Roman and Osborne, Michael A.},
	month = feb,
	year = {2019},
	keywords = {Statistics - Machine Learning, preprint, Computer Science - Machine Learning},
	file = {4279/Chai et al. - 2019 - Automated Model Selection with Bayesian Quadrature.pdf}
}