
@article{nickson_blitzkriging_2015,
	title = {Blitzkriging : {Kronecker}-structured {Stochastic} {Gaussian} {Processes}},
	shorttitle = {Blitzkriging},
	url = {http://arxiv.org/abs/1510.07965},
	abstract = {We present Blitzkriging, a new approach to fast inference for Gaussian processes, applicable to regression, optimisation and classification. State-of-the-art (stochastic) inference for Gaussian processes on very large datasets scales cubically in the number of 'inducing inputs', variables introduced to factorise the model. Blitzkriging shares state-of-the-art scaling with data, but reduces the scaling in the number of inducing points to approximately linear. Further, in contrast to other methods, Blitzkriging: does not force the data to conform to any particular structure (including grid-like); reduces reliance on error-prone optimisation of inducing point locations; and is able to learn rich (covariance) structure from the data. We demonstrate the benefits of our approach on real data in regression, time-series prediction and signal-interpolation experiments.},
	journal = {arXiv:1510.07965 [stat]},
	author = {Nickson, Thomas and Gunter, Tom and Lloyd, Chris and Osborne, Michael A. and Roberts, Stephen},
	month = oct,
	year = {2015},
	keywords = {preprint},
	file = {Nickson et al_2015_Blitzkriging.pdf:/Users/mosb/Google Drive/Zotero/Nickson et al_2015_Blitzkriging.pdf},
}

@techreport{reece_anomaly_2009,
	title = {Anomaly detection and removal using nonstationary {Gaussian} processes},
	abstract = {This paper proposes a novel Gaussian process approach to
fault removal in time-series data. Fault removal does not delete
the faulty signal data but, instead, massages the fault from
the data. We assume that only one fault occurs at any one
time and model the signal by two separate non-parametric
Gaussian process models for both the physical phenomenon
and the fault. In order to facilitate fault removal we introduce
the Markov Region Link kernel for handling non-stationary
Gaussian Processes. This kernel is piece-wise stationary but
guarantees that functions generated by it and their derivatives
(when required) are everywhere continuous. We apply this
kernel to the removal of drift and bias errors in faulty sensor
data and also to the recovery of EOG artifact corrupted EEG
signals.},
	institution = {Department of Engineering Science, University of Oxford},
	author = {Reece, Steve and Garnett, Roman and Osborne, Michael A. and Roberts, Stephen J.},
	year = {2009},
	keywords = {preprint},
	file = {Reece et al_2009_Anomaly detection and removal using nonstationary Gaussian processes.pdf:/Users/mosb/Google Drive/Zotero/Reece et al_2009_Anomaly detection and removal using nonstationary Gaussian processes.pdf},
}

@article{gillani_communication_2014,
	title = {Communication {Communities} in {MOOCs}},
	abstract = {Massive Open Online Courses (MOOCs) bring together thousands of people from different geographies and demographic backgrounds -- but to date, little is known about how they learn or communicate. We introduce a new content-analysed MOOC dataset and use Bayesian Non-negative Matrix Factorization (BNMF) to extract communities of learners based on the nature of their online forum posts. We see that BNMF yields a superior probabilistic generative model for online discussions when compared to other models, and that the communities it learns are differentiated by their composite students' demographic and course performance indicators. These findings suggest that computationally efficient probabilistic generative modelling of MOOCs can reveal important insights for educational researchers and practitioners and help to develop more intelligent and responsive online learning environments.},
	journal = {arXiv preprint arXiv:1403.4640},
	author = {Gillani, Nabeel and Eynon, Rebecca and Osborne, Michael A. and Hjorth, Isis and Roberts, Stephen},
	year = {2014},
	keywords = {preprint},
	file = {Gillani et al_2014_Communication Communities in MOOCs.pdf:/Users/mosb/Google Drive/Zotero/Gillani et al_2014_Communication Communities in MOOCs.pdf},
}

@misc{osborne_epistemic_2008,
	title = {Epistemic {Uncertainty} in {Quantum} {Mechanics}},
	author = {Osborne, Michael A.},
	month = apr,
	year = {2008},
	keywords = {preprint},
	file = {Osborne_2008_Epistemic Uncertainty in Quantum Mechanics.pdf:/Users/mosb/Google Drive/Zotero/Osborne_2008_Epistemic Uncertainty in Quantum Mechanics.pdf},
}

@article{hutter_kernel_2013,
	title = {A {Kernel} for {Hierarchical} {Parameter} {Spaces}},
	url = {http://arxiv.org/abs/1310.5738},
	abstract = {We define a family of kernels for mixed continuous/discrete hierarchical parameter spaces and show that they are positive definite.},
	journal = {arXiv preprint arXiv:1310.5738},
	author = {Hutter, Frank and Osborne, Michael A.},
	year = {2013},
	keywords = {preprint},
	file = {Hutter_Osborne_2013_A Kernel for Hierarchical Parameter Spaces.pdf:/Users/mosb/Google Drive/Zotero/Hutter_Osborne_2013_A Kernel for Hierarchical Parameter Spaces.pdf},
}

@techreport{osborne_gaussian_2007,
	title = {Gaussian processes for prediction},
	abstract = {We propose a powerful prediction algorithm built upon Gaussian processes (GPs). They are
particularly useful for their flexibility, facilitating accurate prediction even in the absence of strong
physical models.
GPs further allow us to work within a complete Bayesian probabilistic framework. As such, we
show how the hyperparameters of our system can be marginalised by use of Bayesian Monte Carlo, a
principled method of approximate integration. We employ the error bars of our GP’s predictions as
a means to select only the most informative data to store. This allows us to introduce an iterative
formulation of the GP to give a dynamic, on-line algorithm. We also show how our error bars can be
used to perform active data selection, allowing the GP to select where and when it should next take a
measurement.
We demonstrate how our methods can be applied to multi-sensor prediction problems where data
may be missing, delayed and/or correlated. In particular, we present a real network of weather sensors
as a testbed for our algorithm.},
	number = {PARG-07-01},
	institution = {Department of Engineering Science, University of Oxford},
	author = {Osborne, Michael A. and Roberts, Stephen J.},
	year = {2007},
	keywords = {preprint},
	file = {Osborne_Roberts_2007_Gaussian processes for prediction.pdf:/Users/mosb/Google Drive/Zotero/Osborne_Roberts_2007_Gaussian processes for prediction.pdf},
}

@article{nickson_automated_2014,
	title = {Automated {Machine} {Learning} on {Big} {Data} using {Stochastic} {Algorithm} {Tuning}},
	url = {http://arxiv.org/abs/1407.7969},
	abstract = {We introduce a means of automating machine learning (ML) for big data tasks, by performing scalable stochastic Bayesian optimisation of ML algorithm parameters and hyper-parameters. More often than not, the critical tuning of ML algorithm parameters has relied on domain expertise from experts, along with laborious hand-tuning, brute search or lengthy sampling runs. Against this background, Bayesian optimisation is finding increasing use in automating parameter tuning, making ML algorithms accessible even to non-experts. However, the state of the art in Bayesian optimisation is incapable of scaling to the large number of evaluations of algorithm performance required to fit realistic models to complex, big data. We here describe a stochastic, sparse, Bayesian optimisation strategy to solve this problem, using many thousands of noisy evaluations of algorithm performance on subsets of data in order to effectively train algorithms for big data. We provide a comprehensive benchmarking of possible sparsification strategies for Bayesian optimisation, concluding that a Nystrom approximation offers the best scaling and performance for real tasks. Our proposed algorithm demonstrates substantial improvement over the state of the art in tuning the parameters of a Gaussian Process time series prediction task on real, big data.},
	journal = {arXiv preprint arXiv:1407.7969},
	author = {Nickson, Thomas and Osborne, Michael A. and Reece, Steven and Roberts, Stephen J.},
	year = {2014},
	note = {https://is.gd/e3JAVg},
	keywords = {preprint},
	file = {Nickson et al_2014_Automated Machine Learning on Big Data using Stochastic Algorithm Tuning.pdf:/Users/mosb/Google Drive/Zotero/Nickson et al_2014_Automated Machine Learning on Big Data using Stochastic Algorithm Tuning.pdf},
}

@article{salas_variational_2015,
	title = {A {Variational} {Bayesian} {State}-{Space} {Approach} to {Online} {Passive}-{Aggressive} {Regression}},
	url = {http://arxiv.org/abs/1509.02438},
	abstract = {Online Passive-Aggressive (PA) learning is a class of online margin-based algorithms suitable for a wide range of real-time prediction tasks, including classification and regression. PA algorithms are formulated in terms of deterministic point-estimation problems governed by a set of user-defined hyperparameters: the approach fails to capture model/prediction uncertainty and makes their performance highly sensitive to hyperparameter configurations. In this paper, we introduce a novel PA learning framework for regression that overcomes the above limitations. We contribute a Bayesian state-space interpretation of PA regression, along with a novel online variational inference scheme, that not only produces probabilistic predictions, but also offers the benefit of automatic hyperparameter tuning. Experiments with various real-world data sets show that our approach performs significantly better than a more standard, linear Gaussian state-space model.},
	journal = {arXiv:1509.02438 [stat]},
	author = {Salas, Arnold and Roberts, Stephen J. and Osborne, Michael A.},
	month = sep,
	year = {2015},
	keywords = {preprint},
	file = {Salas et al_2015_A Variational Bayesian State-Space Approach to Online Passive-Aggressive.pdf:/Users/mosb/Google Drive/Zotero/Salas et al_2015_A Variational Bayesian State-Space Approach to Online Passive-Aggressive.pdf},
}

@article{rizvi_novel_2017,
	title = {A {Novel} {Approach} to {Forecasting} {Financial} {Volatility} with {Gaussian} {Process} {Envelopes}},
	url = {https://arxiv.org/abs/1705.00891},
	abstract = {In this paper we use Gaussian Process (GP) regression to propose a novel approach for predicting volatility of financial returns by forecasting the envelopes of the time series. We provide a direct comparison of their performance to traditional approaches such as GARCH. We compare the forecasting power of three approaches: GP regression on the absolute and squared returns; regression on the envelope of the returns and the absolute returns; and regression on the envelope of the negative and positive returns separately. We use a maximum a posteriori estimate with a Gaussian prior to determine our hyperparameters. We also test the effect of hyperparameter updating at each forecasting step. We use our approaches to forecast out-of-sample volatility of four currency pairs over a 2 year period, at half-hourly intervals. From three kernels, we select the kernel giving the best performance for our data. We use two published accuracy measures and four statistical loss functions to evaluate the forecasting ability of GARCH vs GPs. In mean squared error the GP's perform 20\% better than a random walk model, and 50\% better than GARCH for the same data.},
	urldate = {2017-06-21},
	journal = {arXiv preprint arXiv:1705.00891},
	author = {Rizvi, Syed Ali Asad and Roberts, Stephen J. and Osborne, Michael A. and Nyikosa, Favour},
	year = {2017},
	keywords = {preprint},
	file = {Rizvi et al_2017_A Novel Approach to Forecasting Financial Volatility with Gaussian Process.pdf:/Users/mosb/Google Drive/Zotero/Rizvi et al_2017_A Novel Approach to Forecasting Financial Volatility with Gaussian Process.pdf},
}

@article{mcleod_practical_2017,
	title = {Practical {Bayesian} {Optimization} for {Variable} {Cost} {Objectives}},
	url = {http://arxiv.org/abs/1703.04335},
	abstract = {We propose a novel Bayesian Optimization approach for black-box functions with an environmental variable whose value determines the tradeoff between evaluation cost and the fidelity of the evaluations. Further, we use a novel approach to sampling support points, allowing faster construction of the acquisition function. This allows us to achieve optimization with lower overheads than previous approaches and is implemented for a more general class of problem. We show this approach to be effective on synthetic and real world benchmark problems.},
	urldate = {2017-06-30},
	journal = {arXiv:1703.04335 [stat]},
	author = {McLeod, Mark and Osborne, Michael A. and Roberts, Stephen J.},
	month = mar,
	year = {2017},
	note = {https://github.com/markm541374/gpbo},
	keywords = {preprint},
	file = {McLeod et al_2017_Practical Bayesian Optimization for Variable Cost Objectives.pdf:/Users/mosb/Google Drive/Zotero/McLeod et al_2017_Practical Bayesian Optimization for Variable Cost Objectives.pdf},
}

@article{rontsis_distributionally_2017,
	title = {Distributionally {Ambiguous} {Optimization} {Techniques} in {Batch} {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1707.04191},
	abstract = {We propose a novel, theoretically-grounded, acquisition function for batch Bayesian optimization informed by insights from distributionally ambiguous optimization. Our acquisition function is a lower bound on the well-known Expected Improvement function -- which requires a multi-dimensional Gaussian Expectation over a piecewise affine function -- and is computed by evaluating instead the best-case expectation over all probability distributions consistent with the same mean and variance as the original Gaussian distribution. Unlike alternative approaches including Expected Improvement, our proposed acquisition function avoids multi-dimensional integrations entirely, and can be computed exactly as the solution of a convex optimization problem in the form of a tractable semidefinite program (SDP). Moreover, we prove that the solution of this SDP also yields exact numerical derivatives, which enable efficient optimization of the acquisition function. Finally, it efficiently handles marginalized posteriors with respect to the Gaussian Process' hyperparameters. We demonstrate superior performance to heuristic alternatives and approximations of the intractable expected improvement, justifying this performance difference based on simple examples that break the assumptions of state-of-the-art methods.},
	urldate = {2017-10-31},
	journal = {arXiv:1707.04191 [stat]},
	author = {Rontsis, Nikitas and Osborne, Michael A. and Goulart, Paul J.},
	month = jul,
	year = {2017},
	note = {https://github.com/oxfordcontrol/Bayesian-Optimization},
	keywords = {Statistics - Machine Learning, preprint},
	file = {Rontsis et al_2017_Distributionally Ambiguous Optimization Techniques in Batch Bayesian.pdf:/Users/mosb/Google Drive/Zotero/Rontsis et al_2017_Distributionally Ambiguous Optimization Techniques in Batch Bayesian.pdf},
}

@article{zhao_quantum_2018,
	title = {Quantum algorithms for training {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1803.10520},
	abstract = {Gaussian processes (GPs) are important models in supervised machine learning. Training in Gaussian processes refers to selecting the covariance functions and the associated parameters in order to improve the outcome of predictions, the core of which amounts to evaluating the logarithm of the marginal likelihood (LML) of a given model. LML gives a concrete measure of the quality of prediction that a GP model is expected to achieve. The classical computation of LML typically carries a polynomial time overhead with respect to the input size. We propose a quantum algorithm that computes the logarithm of the determinant of a Hermitian matrix, which runs in logarithmic time for sparse matrices. This is applied in conjunction with a variant of the quantum linear system algorithm that allows for logarithmic time computation of the form \${\textbackslash}mathbf\{y\}{\textasciicircum}TA{\textasciicircum}\{-1\}{\textbackslash}mathbf\{y\}\$, where \${\textbackslash}mathbf\{y\}\$ is a dense vector and \$A\$ is the covariance matrix. We hence show that quantum computing can be used to estimate the LML of a GP with exponentially improved efficiency under certain conditions.},
	urldate = {2018-03-29},
	journal = {arXiv:1803.10520 [quant-ph, stat]},
	author = {Zhao, Zhikuan and Fitzsimons, Jack K. and Osborne, Michael A. and Roberts, Stephen J. and Fitzsimons, Joseph F.},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Learning, Quantum Physics, Statistics - Machine Learning, preprint},
	file = {Zhao et al_2018_Quantum algorithms for training Gaussian Processes.pdf:/Users/mosb/Google Drive/Zotero/Zhao et al_2018_Quantum algorithms for training Gaussian Processes.pdf},
}

@article{nyikosa_bayesian_2018,
	title = {Bayesian {Optimization} for {Dynamic} {Problems}},
	url = {http://arxiv.org/abs/1803.03432},
	abstract = {We propose practical extensions to Bayesian optimization for solving dynamic problems. We model dynamic objective functions using spatiotemporal Gaussian process priors which capture all the instances of the functions over time. Our extensions to Bayesian optimization use the information learnt from this model to guide the tracking of a temporally evolving minimum. By exploiting temporal correlations, the proposed method also determines when to make evaluations, how fast to make those evaluations, and it induces an appropriate budget of steps based on the available information. Lastly, we evaluate our technique on synthetic and real-world problems.},
	urldate = {2018-04-16},
	journal = {arXiv:1803.03432 [stat]},
	author = {Nyikosa, Favour M. and Osborne, Michael A. and Roberts, Stephen J.},
	month = mar,
	year = {2018},
	keywords = {Statistics - Machine Learning, preprint},
	file = {Nyikosa et al_2018_Bayesian Optimization for Dynamic Problems.pdf:/Users/mosb/Google Drive/Zotero/Nyikosa et al_2018_Bayesian Optimization for Dynamic Problems.pdf},
}

@article{granziol_entropic_2018,
	title = {Entropic {Spectral} {Learning} in {Large} {Scale} {Networks}},
	url = {http://arxiv.org/abs/1804.06802},
	abstract = {We present a novel algorithm for learning the spectral density of large scale networks using stochastic trace estimation and the method of maximum entropy. The complexity of the algorithm is linear in the number of non-zero elements of the matrix, offering a computational advantage over other algorithms. We apply our algorithm to the problem of community detection in large networks. We show state-of-the-art performance on both synthetic and real datasets.},
	urldate = {2018-04-19},
	journal = {arXiv:1804.06802 [cs, math, stat]},
	author = {Granziol, Diego and Ru, Binxin and Zohren, Stefan and Dong, Xiaowen and Osborne, Michael and Roberts, Stephen},
	month = apr,
	year = {2018},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, preprint, Computer Science - Information Theory},
	file = {Granziol et al_2018_Entropic Spectral Learning in Large Scale Networks.pdf:/Users/mosb/Google Drive/Zotero/Granziol et al_2018_Entropic Spectral Learning in Large Scale Networks.pdf},
}

@article{paul_contextual_2018,
	title = {Contextual {Policy} {Optimisation}},
	url = {http://arxiv.org/abs/1805.10662},
	abstract = {Policy gradient methods have been successfully applied to a variety of reinforcement learning tasks. However, while learning in a simulator, these methods do not utilise the opportunity to improve learning by adjusting certain environment variables: unobservable state features that are randomly determined by the environment in a physical setting, but that are controllable in a simulator. This can lead to slow learning, or convergence to highly suboptimal policies. In this paper, we present contextual policy optimisation (CPO). The central idea is to use Bayesian optimisation to actively select the distribution of the environment variable that maximises the improvement generated by each iteration of the policy gradient method. To make this Bayesian optimisation practical, we contribute two easy-to-compute low-dimensional fingerprints of the current policy. We apply CPO to a number of continuous control tasks of varying difficulty and show that CPO can efficiently learn policies that are robust to significant rare events, which are unlikely to be observable under random sampling but are key to learning good policies.},
	urldate = {2018-05-29},
	journal = {arXiv:1805.10662 [cs, stat]},
	author = {Paul, Supratik and Osborne, Michael A. and Whiteson, Shimon},
	month = may,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning, preprint},
	file = {Paul et al_2018_Contextual Policy Optimisation.pdf:/Users/mosb/Google Drive/Zotero/Paul et al_2018_Contextual Policy Optimisation.pdf},
}

@article{nyikosa_adaptive_2019,
	title = {Adaptive {Configuration} {Oracle} for {Online} {Portfolio} {Selection} {Methods}},
	url = {http://arxiv.org/abs/1908.08258},
	abstract = {Financial markets are complex environments that produce enormous amounts of noisy and non-stationary data. One fundamental problem is online portfolio selection, the goal of which is to exploit this data to sequentially select portfolios of assets to achieve positive investment outcomes while managing risks. Various algorithms have been proposed for solving this problem in ﬁelds such as ﬁnance, statistics and machine learning, among others. Most of the methods have parameters that are estimated from backtests for good performance. Since these algorithms operate on nonstationary data that reﬂects the complexity of ﬁnancial markets, we posit that adaptively tuning these parameters in an intelligent manner is a remedy for dealing with this complexity. In this paper, we model the mapping between the parameter space and the space of performance metrics using a Gaussian process prior. We then propose an oracle based on adaptive Bayesian optimization for automatically and adaptively conﬁguring online portfolio selection methods. We test the efﬁcacy of our solution on algorithms operating on equity and index data from various markets.},
	language = {en},
	urldate = {2019-09-29},
	journal = {arXiv:1908.08258 [cs, stat]},
	author = {Nyikosa, Favour M. and Osborne, Michael A. and Roberts, Stephen J.},
	month = aug,
	year = {2019},
	keywords = {G.3, Statistics - Machine Learning, preprint, Computer Science - Machine Learning, 62P30},
	file = {Nyikosa et al_2019_Adaptive Configuration Oracle for Online Portfolio Selection Methods.pdf:/Users/mosb/Google Drive/Zotero/Nyikosa et al_2019_Adaptive Configuration Oracle for Online Portfolio Selection Methods.pdf},
}

@article{hamid_marginalising_2021,
	title = {Marginalising over {Stationary} {Kernels} with {Bayesian} {Quadrature}},
	url = {http://arxiv.org/abs/2106.07452},
	abstract = {Marginalising over families of Gaussian Process kernels produces flexible model classes with well-calibrated uncertainty estimates. Existing approaches require likelihood evaluations of many kernels, rendering them prohibitively expensive for larger datasets. We propose a Bayesian Quadrature scheme to make this marginalisation more efficient and thereby more practical. Through use of the maximum mean discrepancies between distributions, we define a kernel over kernels that captures invariances between Spectral Mixture (SM) Kernels. Kernel samples are selected by generalising an information-theoretic acquisition function for warped Bayesian Quadrature. We show that our framework achieves more accurate predictions with better calibrated uncertainty than state-of-the-art baselines, especially when given limited (wall-clock) time budgets.},
	urldate = {2021-08-11},
	journal = {arXiv:2106.07452 [cs, stat]},
	author = {Hamid, Saad and Schulze, Sebastian and Osborne, Michael A. and Roberts, Stephen J.},
	month = jun,
	year = {2021},
	keywords = {Computer Science - Machine Learning, preprint, Statistics - Machine Learning},
	file = {Hamid et al_2021_Marginalising over Stationary Kernels with Bayesian Quadrature.pdf:/Users/mosb/Google Drive/Zotero/Hamid et al_2021_Marginalising over Stationary Kernels with Bayesian Quadrature.pdf},
}

@article{bueren_personalized_2021,
	title = {Personalized {Closed}-{Loop} {Brain} {Stimulation} for {Effective} {Neurointervention} {Across} {Participants}},
	copyright = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.03.18.436018v1},
	abstract = {Accumulating evidence from human-based research has highlighted that the prevalent one-size-fits-all approach for neural and behavioral interventions is inefficient. This approach can benefit one individual, but be ineffective or even detrimental for another. Studying the efficacy of the large range of different parameters for different individuals is costly, time-consuming and requires a large sample size that makes such research impractical and hinders effective interventions. Here an active machine learning technique is presented across participants—personalized Bayesian optimization (pBO)—that searches available parameter combinations to optimize an intervention as a function of an individual’s ability. This novel technique was utilized to identify transcranial alternating current stimulation frequency and current strength combinations most likely to improve arithmetic performance, based on a subject’s baseline arithmetic abilities. The pBO was performed across all subjects tested, building a model of subject performance, capable of recommending parameters for future subjects based on their baseline arithmetic ability. pBO successfully searches, learns, and recommends parameters for an effective neurointervention as supported by behavioral, stimulation, and neural data. The application of pBO in human-based research opens up new avenues for personalized and more effective interventions, as well as discoveries of protocols for treatment and translation to other clinical and non-clinical domains.},
	language = {en},
	urldate = {2021-08-11},
	author = {Bueren, Nienke E. R. van and Reed, Thomas L. and Nguyen, Vu and Sheffield, James G. and Ven, Sanne H. G. van der and Osborne, Michael A. and Kroesbergen, Evelyn H. and Kadosh, Roi Cohen},
	month = mar,
	year = {2021},
	keywords = {preprint},
	pages = {2021.03.18.436018},
	file = {Bueren et al_2021_Personalized Closed-Loop Brain Stimulation for Effective Neurointervention.pdf:/Users/mosb/Google Drive/Zotero/Bueren et al_2021_Personalized Closed-Loop Brain Stimulation for Effective Neurointervention.pdf},
}

@misc{adachi_quadrature_2024,
	title = {A {Quadrature} {Approach} for {General}-{Purpose} {Batch} {Bayesian} {Optimization} via {Probabilistic} {Lifting}},
	url = {https://doi.org/10.48550/arXiv.2404.12219},
	doi = {10.48550/arXiv.2404.12219},
	abstract = {Parallelisation in Bayesian optimisation is a common strategy but faces several challenges: the need for flexibility in acquisition functions and kernel choices, flexibility dealing with discrete and continuous variables simultaneously, model misspecification, and lastly fast massive parallelisation. To address these challenges, we introduce a versatile and modular framework for batch Bayesian optimisation via probabilistic lifting with kernel quadrature, called SOBER, which we present as a Python library based on GPyTorch/BoTorch. Our framework offers the following unique benefits: (1) Versatility in downstream tasks under a unified approach. (2) A gradient-free sampler, which does not require the gradient of acquisition functions, offering domain-agnostic sampling (e.g., discrete and mixed variables, non-Euclidean space). (3) Flexibility in domain prior distribution. (4) Adaptive batch size (autonomous determination of the optimal batch size). (5) Robustness against a misspecified reproducing kernel Hilbert space. (6) Natural stopping criterion.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Adachi, Masaki and Hayakawa, Satoshi and Jørgensen, Martin and Hamid, Saad and Oberhauser, Harald and Osborne, Michael A.},
	month = apr,
	year = {2024},
	keywords = {62C10, 62F15, Computer Science - Machine Learning, Mathematics - Numerical Analysis, preprint, Statistics - Machine Learning},
	file = {6654/Adachi et al. - 2024 - A Quadrature Approach for General-Purpose Batch Ba.pdf},
}

@misc{heim_governing_2024,
	title = {Governing {Through} the {Cloud}: {The} {Intermediary} {Role} of {Compute} {Providers} in {AI} {Regulation}},
	shorttitle = {Governing {Through} the {Cloud}},
	url = {https://doi.org/10.48550/arXiv.2403.08501},
	doi = {10.48550/arXiv.2403.08501},
	abstract = {As jurisdictions around the world take their first steps toward regulating the most powerful AI systems, such as the EU AI Act and the US Executive Order 14110, there is a growing need for effective enforcement mechanisms that can verify compliance and respond to violations. We argue that compute providers should have legal obligations and ethical responsibilities associated with AI development and deployment, both to provide secure infrastructure and to serve as intermediaries for AI regulation. Compute providers can play an essential role in a regulatory ecosystem via four key capacities: as securers, safeguarding AI systems and critical infrastructure; as record keepers, enhancing visibility for policymakers; as verifiers of customer activities, ensuring oversight; and as enforcers, taking actions against rule violations. We analyze the technical feasibility of performing these functions in a targeted and privacy-conscious manner and present a range of technical instruments. In particular, we describe how non-confidential information, to which compute providers largely already have access, can provide two key governance-relevant properties of a computational workload: its type-e.g., large-scale training or inference-and the amount of compute it has consumed. Using AI Executive Order 14110 as a case study, we outline how the US is beginning to implement record keeping requirements for compute providers. We also explore how verification and enforcement roles could be added to establish a comprehensive AI compute oversight scheme. We argue that internationalization will be key to effective implementation, and highlight the critical challenge of balancing confidentiality and privacy with risk mitigation as the role of compute providers in AI regulation expands.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Heim, Lennart and Fist, Tim and Egan, Janet and Huang, Sihao and Zekany, Stephen and Trager, Robert and Osborne, Michael A. and Zilberman, Noa},
	month = mar,
	year = {2024},
	keywords = {Computer Science - Computers and Society, preprint},
	file = {6662/Heim et al. - 2024 - Governing Through the Cloud The Intermediary Role.pdf},
}

@misc{ziomek_beyond_2024,
	title = {Beyond {Lengthscales}: {No}-regret {Bayesian} {Optimisation} {With} {Unknown} {Hyperparameters} {Of} {Any} {Type}},
	shorttitle = {Beyond {Lengthscales}},
	url = {https://doi.org/10.48550/arXiv.2402.01632},
	doi = {10.48550/arXiv.2402.01632},
	abstract = {Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known. The commonly used maximum likelihood estimator for hyperparameters of the Gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in Bayesian optimisation. Since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the Gaussian process fit, theoretical analysis of Bayesian optimisation with unknown hyperparameters is very challenging. Previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel Hilbert space norm and applied only to the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparameters of arbitrary form, and which supports both Bayesian and frequentist settings. Our proof idea is novel and can easily be extended to other variants of Bayesian optimisation. We show this by extending our algorithm to the adversarially robust optimisation setting under unknown hyperparameters. Finally, we empirically evaluate our algorithm on a set of toy problems and show that it can outperform the maximum likelihood estimator.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Ziomek, Juliusz and Adachi, Masaki and Osborne, Michael A.},
	month = feb,
	year = {2024},
	keywords = {Computer Science - Machine Learning, preprint, Statistics - Machine Learning},
	file = {6667/Ziomek et al. - 2024 - Beyond Lengthscales No-regret Bayesian Optimisati.pdf},
}

@misc{papamarkou_position_2024,
	title = {Position {Paper}: {Bayesian} {Deep} {Learning} in the {Age} of {Large}-{Scale} {AI}},
	shorttitle = {Position {Paper}},
	url = {https://doi.org/10.48550/arXiv.2402.00809},
	doi = {10.48550/arXiv.2402.00809},
	abstract = {In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Papamarkou, Theodore and Skoularidou, Maria and Palla, Konstantina and Aitchison, Laurence and Arbel, Julyan and Dunson, David and Filippone, Maurizio and Fortuin, Vincent and Hennig, Philipp and Lobato, Jose Miguel Hernandez and Hubin, Aliaksandr and Immer, Alexander and Karaletsos, Theofanis and Khan, Mohammad Emtiyaz and Kristiadi, Agustinus and Li, Yingzhen and Mandt, Stephan and Nemeth, Christopher and Osborne, Michael A. and Rudner, Tim G. J. and Rügamer, David and Teh, Yee Whye and Welling, Max and Wilson, Andrew Gordon and Zhang, Ruqi},
	month = feb,
	year = {2024},
	keywords = {Computer Science - Machine Learning, preprint, Statistics - Machine Learning},
	file = {6670/Papamarkou et al. - 2024 - Position Paper Bayesian Deep Learning in the Age .pdf},
}

@misc{adachi_sober_2023,
	title = { {SOBER}: {Highly} {Parallel} {Bayesian} {Optimization} and {Bayesian} {Quadrature} over {Discrete} and {Mixed} {Spaces}},
	shorttitle = { {SOBER}},
	url = {https://doi.org/10.48550/arXiv.2301.11832},
	doi = {10.48550/arXiv.2301.11832},
	abstract = {Batch Bayesian optimisation and Bayesian quadrature have been shown to be sample-efficient methods of performing optimisation and quadrature where expensive-to-evaluate objective functions can be queried in parallel. However, current methods do not scale to large batch sizes -- a frequent desideratum in practice (e.g. drug discovery or simulation-based inference). We present a novel algorithm, SOBER, which permits scalable and diversified batch global optimisation and quadrature with arbitrary acquisition functions and kernels over discrete and mixed spaces. The key to our approach is to reformulate batch selection for global optimisation as a quadrature problem, which relaxes acquisition function maximisation (non-convex) to kernel recombination (convex). Bridging global optimisation and quadrature can efficiently solve both tasks by balancing the merits of exploitative Bayesian optimisation and explorative Bayesian quadrature. We show that SOBER outperforms 11 competitive baselines on 12 synthetic and diverse real-world tasks.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Adachi, Masaki and Hayakawa, Satoshi and Hamid, Saad and Jørgensen, Martin and Oberhauser, Harald and Osborne, Micheal A.},
	month = jul,
	year = {2023},
	note = {https://github.com/ma921/SOBER/},
	keywords = {62C10, 62F15, Computer Science - Machine Learning, Mathematics - Numerical Analysis, preprint, Statistics - Computation, Statistics - Machine Learning},
	file = {6683/Adachi et al. - 2023 - SOBER Highly Parallel Bayesian Optimization and B.pdf},
}

@misc{hamid_bayesian_2023,
	title = {Bayesian {Quadrature} for {Neural} {Ensemble} {Search}},
	url = {https://doi.org/10.48550/arXiv.2303.08874},
	doi = {10.48550/arXiv.2303.08874},
	abstract = {Ensembling can improve the performance of Neural Networks, but existing approaches struggle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore, existing methods construct equally weighted ensembles, and this is likely to be vulnerable to the failure modes of the weaker architectures. By viewing ensembling as approximately marginalising over architectures we construct ensembles using the tools of Bayesian Quadrature -- tools which are well suited to the exploration of likelihood surfaces with dispersed, narrow peaks. Additionally, the resulting ensembles consist of architectures weighted commensurate with their performance. We show empirically -- in terms of test likelihood, accuracy, and expected calibration error -- that our method outperforms state-of-the-art baselines, and verify via ablation studies that its components do so independently.},
	urldate = {2024-05-24},
	publisher = {arXiv},
	author = {Hamid, Saad and Wan, Xingchen and Jørgensen, Martin and Ru, Binxin and Osborne, Michael},
	month = mar,
	year = {2023},
	note = {https://github.com/saadhamidml/bq-nes},
	keywords = {Computer Science - Machine Learning, preprint, Statistics - Machine Learning},
	file = {6693/Hamid et al. - 2023 - Bayesian Quadrature for Neural Ensemble Search.pdf},
}

@misc{nguyen_gaussian_2022,
	title = {Gaussian {Process} {Sampling} and {Optimization} with {Approximate} {Upper} and {Lower} {Bounds}},
	url = {https://doi.org/10.48550/arXiv.2110.12087},
	doi = {10.48550/arXiv.2110.12087},
	abstract = {Many functions have approximately-known upper and/or lower bounds, potentially aiding the modeling of such functions. In this paper, we introduce Gaussian process models for functions where such bounds are (approximately) known. More specifically, we propose the first use of such bounds to improve Gaussian process (GP) posterior sampling and Bayesian optimization (BO). That is, we transform a GP model satisfying the given bounds, and then sample and weight functions from its posterior. To further exploit these bounds in BO settings, we present bounded entropy search (BES) to select the point gaining the most information about the underlying function, estimated by the GP samples, while satisfying the output constraints. We characterize the sample variance bounds and show that the decision made by BES is explainable. Our proposed approach is conceptually straightforward and can be used as a plug in extension to existing methods for GP posterior sampling and Bayesian optimization.},
	urldate = {2024-05-24},
	publisher = {arXiv},
	author = {Nguyen, Vu and Deisenroth, Marc Peter and Osborne, Michael A.},
	month = oct,
	year = {2022},
	keywords = {Computer Science - Machine Learning, preprint, Statistics - Machine Learning},
	file = {6724/Nguyen et al. - 2022 - Gaussian Process Sampling and Optimization with Ap.pdf},
}

@misc{severin_cross-architecture_2021,
	title = {Cross-architecture {Tuning} of {Silicon} and {SiGe}-based {Quantum} {Devices} {Using} {Machine} {Learning}},
	url = {https://doi.org/10.48550/arXiv.2107.12975},
	doi = {10.48550/arXiv.2107.12975},
	abstract = {The potential of Si and SiGe-based devices for the scaling of quantum circuits is tainted by device variability. Each device needs to be tuned to operation conditions. We give a key step towards tackling this variability with an algorithm that, without modification, is capable of tuning a 4-gate Si FinFET, a 5-gate GeSi nanowire and a 7-gate SiGe heterostructure double quantum dot device from scratch. We achieve tuning times of 30, 10, and 92 minutes, respectively. The algorithm also provides insight into the parameter space landscape for each of these devices. These results show that overarching solutions for the tuning of quantum devices are enabled by machine learning.},
	urldate = {2024-05-24},
	publisher = {arXiv},
	author = {Severin, B. and Lennon, D. T. and Camenzind, L. C. and Vigneau, F. and Fedele, F. and Jirovec, D. and Ballabio, A. and Chrastina, D. and Isella, G. and de Kruijf, M. and Carballido, M. J. and Svab, S. and Kuhlmann, A. V. and Braakman, F. R. and Geyer, S. and Froning, F. N. M. and Moon, H. and Osborne, M. A. and Sejdinovic, D. and Katsaros, G. and Zumbühl, D. M. and Briggs, G. A. D. and Ares, N.},
	month = jul,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Mesoscale and Nanoscale Physics, preprint, Quantum Physics},
	file = {6731/Severin et al. - 2021 - Cross-architecture Tuning of Silicon and SiGe-base.pdf},
}
